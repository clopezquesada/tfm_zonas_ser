{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8040985f",
      "metadata": {
        "id": "8040985f"
      },
      "source": [
        "# TFM - Análisis de las zonas SER de Madrid"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a7e4e4",
      "metadata": {
        "id": "66a7e4e4"
      },
      "source": [
        "## MEMORIA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1db99d",
      "metadata": {
        "id": "0f1db99d"
      },
      "source": [
        "# 1. Introducción\n",
        "Encontrar aparcamiento supone un reto para la mayoría de conductores, este problema se intensifica aún más en grandes núcleos urbanos como Madrid. De hecho, el artículo ... indica que el 76% de los españoles estima que la falta de aparcamiento es el principal problema de movilidad urbana que tienen en su ciudad, además, en éste se destaca que Madrid es una de las ciudades en las que este problema se hace más notable. La dificultad en el aparcamiento se ve aún más intensificada en las zonas céntricas de la capital debido al gran volumen de vehículos y el tráfico constante. Éste hecho no solo conlleva una pérdida de tiempo notable para los usuarios si no también un gasto considerable de combustible y, por ende, una mayor contaminación.\n",
        "\n",
        "En la actualidad, uno de los principales retos es empujar a las ciudades hacia un modelo más sostenible e inteligente, las conocidas como smart cities, así un aparcamiento gestionado de manera eficiente es un reto clave para reducir el impacto medioambiental y para ayudar a la fluidez de la movilidad urbana. Así, este trabajo nace del interés por aplicar la ciencia de datos a un problema que está a la orden del día con gran impacto social y medioambiental. En particular, se pretende resaltar la creciente disponibilidad de datos a tiempo real, que junto con datos históricos y tecnologías en big data y machine learning permiten abordar problemas urbanos desde una perspectiva innovadora, tal y como se ha estudiado a lo largo del máster. Además, el trabajo tiene en el fondo una naturaleza de segmentación de perfiles que se podría extrapolar a otros escenarios como la agrupación de usuarios en plataformas de venta digitales, aportando así valor a las empresas en la toma de decisiones basadas en datos.\n",
        "\n",
        "*Mirar este párrafo que está fatal*El trabajo se centra en desarrollar una herramienta que utilizando datos a tiempo real, además del conocimiento aportado por datos históricos, lleve a cabo una segmentación de las distintas zonas Servicio de Estacionamiento Regulado del centro de Madrid según la facilidad de encontrar aparcamiento en ellas. Para ello, se utilizarán datos abiertos del Ayuntamiento de Madrid sobre tráfico, parquímetros y zonas SER. A lo largo del trabajo se aplican técnicas de análisis descriptivo, aprendizaje estadístico supervisado y no supervisado y análisis predictivo. Además se desarrollará una aplicación web interactiva que permitirá visualizar estos resultados en mapas, buscando así una interpretación rápida, fácil y visual. Además se han considerados diversos artículos relacionados con el aparcamiento inteligente como por ejemplo, … y … que sirven de fundamento para el. El trabajo se estructura en varios capítulos, en primer lugar, se presenta el problema, a continuación, se describe la recopilación y procesamiento de los datos, seguidos por la metodología utilizada para el análisis y el desarrollo del modelo, después, se presentan los resultados obtenidos y la aplicación web desarrollada. Finalmente, se exponen las conclusiones, limitaciones y posibles líneas futuras de trabajo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b88adb",
      "metadata": {
        "id": "a1b88adb"
      },
      "source": [
        "# 2. Descripción de las fuentes y análisis inicial\n",
        "*Hacer referencia al apartado de carga y análisis del anexo*\n",
        "La totalidad de los datos usados en el trabajo han sido extraidos del Portal de Datos Abiertos del Ayuntamiento de Madrid.\n",
        "Por un lado, introducimos los archivos considerados relativos a la información sobre las zonas de Servicio de Estacionamiento Regulado. En primer lugar, se ha considerado el archivo calles-SER-2025.csv, el cual tiene 33731 filas y 12 columnas, cada una de estas filas se corresponde a información de una zona SER determinada. \\textbf{Poner head del df}. Después se considera el archivo parquimetros.csv el cual contiene 6126 filas y 14 columnas de información sobre parquímetros. \\textbf{Poner head del df}. Se utiliza el archivo Primertrimestre2025.csv, el cual contiene 12745646 y 12 columnas, el cual nos indica la duración e importe de cada uno de los tickets sacados en diversos parquímetros para el primer trimestre de 2025 (coger también los del segundo trimestre hasta mayo???) .\\textbf{poner cabecera df}. La granularidad de este conjunto de datos es parquímetro, día fecha y hora, pero para tener una visión más general de estos datos nos quedamos únicamente con el importe medio, la duración media en minutos, y el número de tickets registrados para cada parquímetro en cada franja de tiempo: mañana, mediodia, tarde y noche, y distinguimos entre si se trata de fin de semana o de un día entre semana. De esta manera acabamos un dataframe llamado zonas-ser-sin-info que recoge las zonas ser sobre las que no tenemos información y por otro lado, zonas-ser-info.\n",
        "Por otro lado, se introducen los archivos relativos al tráfico histórico de Madrid, para ello se consideran los archivos 01-2025.csv, 02-2025.csv,... relativos al tráfico medido cada ... minutos en distintos puntos de medida de Madrid centro. Al unir todos esos datos contamos con un total de 18726997 filas y 9 columnas. Los puntos de medida del tráfico los tenemos en el archivo pmed-ubicacion-06-2025.csv. De nuevo, para tener una visualización más simplificada de los datos consideramos la media de intensidad, media de ocupación y media de carga para cada punto de medida, según la franja de tiempo y si es fin de semana o un día entre semana, el resultado es el df trafico-historico. El procesamiento inicial de estos archivos se ha llevado a cabo en pspark para el tratamiento de los grandes volúmenes de datos de manera efectiva.\n",
        "Los datos de tráfico a tiempo real se obtienen mediante un flujo dinámico en formato XML los cuales se actualizan cada 15 minutos en la url y siguen el siguiente esquema:.. De aquí obtenemos el df trafico-actual.\\textbf{Quizás estaría bien mirar la distribución de los datos para cada franja para ver si está bien hecho lo de coger la media, esto va a alargar mucho todo pero así es más completo, quizás interesa más coger la mediana.}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Metodología\n",
        "## Modelo de regresión\n",
        "\n",
        "## Modelo de clustering\n",
        "\n",
        "Tras el modelo de regresión en el que se han estimado las variables ... y ... para el conjunto de datos que no las tenían. Montamos el modelo de clustering mediante el que se pretende estimar la segmentación de las zonas centro de la capital según distintos factores como: .... Así, se ha estudiado dicha segmentación para cada int_tiempo y para fin_de_semana, la cual se ha llevado a cabo mediante K-means, antes se ha llevado a cabo una reducción de la dimensionalidad, esto también se estudia detalladamente para cada subconjunto, y se determina que 4 componentes principales explican más del 90% de la varianza en cada uno de los subconjuntos. Para el algoritmo de clustering los resultados utilizados, que se pueden visualizar más detalladamente en la sección de Código en el anexo, son los siguiente:\n",
        "\n"
      ],
      "metadata": {
        "id": "8u1zS30y4Fl8"
      },
      "id": "8u1zS30y4Fl8"
    },
    {
      "cell_type": "markdown",
      "id": "5acfe5fe",
      "metadata": {
        "id": "5acfe5fe"
      },
      "source": [
        "## CÓDIGO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RombDF_5g40A",
      "metadata": {
        "id": "RombDF_5g40A"
      },
      "source": [
        "### Lectura y análisis de las fuentes. Tratamiento inicial de los datos.\n",
        "\n",
        "Para comenzar, debido a la naturaleza variada y dispersa de los datos que se van a utilizar llevamos a cabo un pretratamiento de éstos. A lo largo de ésta sección se harán modificaciones de los conjuntos de datos a partir de pandas, Spark y geopandas para llegar a los dataframes con los que trabajaremos finalmente. Ésta se cumplimentará con un análisis exploratorio de algunos de los conjuntos de datos que por el tratamiento que se va a realizar, no se puede analizar más adelante. Explicar de manera adecuada y ligera de leer el procedimiento que se lleva a cabo en esta sección. Falta estudiar valores nulos o missing data y duplicados. Poner id a cada zona ser para identificación única que sea cod_barrio, calle, numero_finca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bdwmh1QMYhxQ",
      "metadata": {
        "id": "Bdwmh1QMYhxQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "328Tl3kFYdu0",
      "metadata": {
        "id": "328Tl3kFYdu0"
      },
      "source": [
        "En primer lugar, se ha considerado el archivo calles-SER-2025.csv, el cual tiene 33731 filas y 10 columnas, cada una de estas filas se corresponde a información de una zona SER determinada, la información sobre éste se puede consultar en ... . Procedemos a su carga y análisis, trabajamos inicialmente con pyspark y con pandas según sea necesario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66493d88",
      "metadata": {
        "id": "66493d88"
      },
      "outputs": [],
      "source": [
        "calles_ser = pd.read_csv('calles_SER_2025.csv', encoding = 'latin-1', sep = ';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-lOJAPEqaKhn",
      "metadata": {
        "id": "-lOJAPEqaKhn"
      },
      "outputs": [],
      "source": [
        "calles_ser.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "egjmAE-SZIig",
      "metadata": {
        "id": "egjmAE-SZIig"
      },
      "outputs": [],
      "source": [
        "calles_ser.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SDdekopJe4M1",
      "metadata": {
        "id": "SDdekopJe4M1"
      },
      "source": [
        "Todo esto después, o al menos en este caso sí que se puede mantener pero quizás mejor después. Descripción general de las variables del df:\n",
        "\n",
        "\n",
        "*   Variables numéricas\n",
        "      - numero_plazas\n",
        "*   Variables categóricas\n",
        "      - Consideramos únicamente distrito, barrio, color, bateria_linea\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68I-cZ07oJGp",
      "metadata": {
        "id": "68I-cZ07oJGp"
      },
      "outputs": [],
      "source": [
        "calles_ser['id'] = (calles_ser['cod_barrio'].astype(str) + \"_\" +\n",
        "                    calles_ser['cod_distrito'].astype(str) + \"_\" +\n",
        "                    calles_ser['numero_finca'].astype(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pGykKToDpOha",
      "metadata": {
        "id": "pGykKToDpOha"
      },
      "outputs": [],
      "source": [
        "duplicados = calles_ser[calles_ser.duplicated(keep=False)]\n",
        "duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nn1d9ZEeqEdn",
      "metadata": {
        "id": "nn1d9ZEeqEdn"
      },
      "outputs": [],
      "source": [
        "calles_ser.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calles_ser['color'] = calles_ser['color'].replace({\n",
        "    '077214010 Verde': 'Verde',\n",
        "    '043000255 Azul': 'Azul',\n",
        "    '081209246 Alta Rotación': 'Alta Rotación',\n",
        "    '255140000 Naranja': 'Naranja',\n",
        "    '255000000 Rojo': 'Rojo'\n",
        "})\n"
      ],
      "metadata": {
        "id": "v97DH4y-BPXA"
      },
      "id": "v97DH4y-BPXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "rWZzXhVAeSU2",
      "metadata": {
        "id": "rWZzXhVAeSU2"
      },
      "source": [
        "Vemos que la distribución del número de plazas es muy asimétrica, además de presentar una gran cantidad de outliers, lo cual es esperable en datos reales, que la mayoría de aparcamientos tengan entre 1 y 15 plazas, pero que haya muchos aparcamientos que puedan tener más. Estudiamos a continuación las variables categóricas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZhK3FEhJaaW7",
      "metadata": {
        "id": "ZhK3FEhJaaW7"
      },
      "source": [
        "Interpretación de las variables categóricas. De este dataframe nos interesa conocer la latitud y la longitud de cada zona SER, su color, si se trata de un aparcamiento en línea o en batería y el número de plazas que tiene. Mostrar visualización con el número de zona SER en cada barrio y poner etiqueta de color y símbolo dependiendo de si se trata de un aparcamiento en línea o en batería. Hacer el símbolo más grande conforme más numero_plazas tenga. El sistema de georreferenciación utilizado en los datos de coordenadas es ETRS89."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xvy41DN0aZVM",
      "metadata": {
        "id": "Xvy41DN0aZVM"
      },
      "outputs": [],
      "source": [
        "from pyproj import Transformer\n",
        "\n",
        "transformer = Transformer.from_crs(\"EPSG:25830\", \"EPSG:4326\", always_xy=True)\n",
        "\n",
        "calles_ser['longitud'], calles_ser['latitud'] = transformer.transform(calles_ser['gis_x'].values, calles_ser['gis_y'].values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calles_ser.to_csv(\"calles_ser_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "hmBqaSWEL1-F"
      },
      "id": "hmBqaSWEL1-F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calles_ser = pd.read_csv('calles_ser_df.csv')"
      ],
      "metadata": {
        "id": "i9uA9jMJMW6G"
      },
      "id": "i9uA9jMJMW6G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qF4f-Ssjjf7l",
      "metadata": {
        "id": "qF4f-Ssjjf7l"
      },
      "outputs": [],
      "source": [
        "parquimetros = pd.read_csv('parquimetros.csv', encoding = 'latin-1', sep = ';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B5K4zB7TqLUh",
      "metadata": {
        "id": "B5K4zB7TqLUh"
      },
      "outputs": [],
      "source": [
        "duplicados = parquimetros[parquimetros.duplicated(keep=False)]\n",
        "duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sEVk0rF46fX0",
      "metadata": {
        "id": "sEVk0rF46fX0"
      },
      "outputs": [],
      "source": [
        "parquimetros.drop_duplicates(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rd1qGSNYmBl4",
      "metadata": {
        "id": "Rd1qGSNYmBl4"
      },
      "outputs": [],
      "source": [
        "parquimetros.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "up0_i2Cql-WB",
      "metadata": {
        "id": "up0_i2Cql-WB"
      },
      "source": [
        "Quitamos los parquímetros que están dados de baja, vamos a interesarnos únicamente por los que están activos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YqWzAofqljld",
      "metadata": {
        "id": "YqWzAofqljld"
      },
      "outputs": [],
      "source": [
        "parquimetros['matricula'] = parquimetros['matricula'].astype('Int64')\n",
        "parquimetros['matricula'] = parquimetros['matricula'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daE1zEP_mSmV",
      "metadata": {
        "id": "daE1zEP_mSmV"
      },
      "outputs": [],
      "source": [
        "parquimetros = parquimetros[parquimetros['fecha_de_baja'].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDx-q04zmU7J",
      "metadata": {
        "id": "hDx-q04zmU7J"
      },
      "outputs": [],
      "source": [
        "parquimetros = parquimetros.drop(columns = ['fecha_de_baja'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uGJOIqcrmtU3",
      "metadata": {
        "id": "uGJOIqcrmtU3"
      },
      "outputs": [],
      "source": [
        "parquimetros.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4pV7rJpVms0H",
      "metadata": {
        "id": "4pV7rJpVms0H"
      },
      "outputs": [],
      "source": [
        "parquimetros.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l5H8XI6mm8je",
      "metadata": {
        "id": "l5H8XI6mm8je"
      },
      "outputs": [],
      "source": [
        "parquimetros['barrio'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xD91WUbsnDj4",
      "metadata": {
        "id": "xD91WUbsnDj4"
      },
      "outputs": [],
      "source": [
        "calles_ser_parquimetro = pd.merge(parquimetros[['matricula','cod_distrito','cod_barrio','numero_finca','calle']],\n",
        "                                  calles_ser[['bateria_linea','color','latitud','longitud','cod_distrito','cod_barrio','numero_finca','calle','numero_plazas']],\n",
        "                                  on=['cod_distrito', 'cod_barrio', 'numero_finca','calle'],\n",
        "                                  how = 'inner',\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gvWUsehCnc37",
      "metadata": {
        "id": "gvWUsehCnc37"
      },
      "outputs": [],
      "source": [
        "calles_ser_parquimetro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CZGpU-ZJqya9",
      "metadata": {
        "id": "CZGpU-ZJqya9"
      },
      "outputs": [],
      "source": [
        "# duplicados = calles_ser_parquimetro[calles_ser_parquimetro.duplicated(keep=False)]\n",
        "# duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I1i3jpOIq1ST",
      "metadata": {
        "id": "I1i3jpOIq1ST"
      },
      "outputs": [],
      "source": [
        "# calles_ser_parquimetro.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XvSXi0lfe9pJ",
      "metadata": {
        "id": "XvSXi0lfe9pJ"
      },
      "source": [
        "Leemos los archivos que contienen la información de los tiquets de los parquímetros, como son archivos de millones filas y muchas columnas usamos Spark para su lectura y posteriores transformaciones hasta obtener un df adecuado para tratar con pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "igcGqcLShE3B",
      "metadata": {
        "id": "igcGqcLShE3B"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import dayofmonth, month, col, count\n",
        "from pyspark.sql.functions import to_timestamp, date_format, when\n",
        "from pyspark.sql.functions import avg, count\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from pyspark.sql.types import FloatType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oypz119jP4Om",
      "metadata": {
        "id": "Oypz119jP4Om"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TiquetsParking\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "tiquets_1trimestre = spark.read.csv(\"Primertrimestre2025.csv\", header=True, inferSchema=True, sep=\";\")\n",
        "# tiquets_2trimestre = spark.read.csv(\"Primertrimestre2025.csv\", header=True, inferSchema=True, sep=\";\")\n",
        "\n",
        "tiquets = tiquets_1trimestre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wVG3adUchXnz",
      "metadata": {
        "id": "wVG3adUchXnz"
      },
      "outputs": [],
      "source": [
        "tiquets.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qZ7m8L0Xha-1",
      "metadata": {
        "id": "qZ7m8L0Xha-1"
      },
      "outputs": [],
      "source": [
        "tiquets.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X-DNdymwhjS0",
      "metadata": {
        "id": "X-DNdymwhjS0"
      },
      "outputs": [],
      "source": [
        "tiquets = tiquets.withColumn(\n",
        "    \"importe_tique\",\n",
        "    regexp_replace(\"importe_tique\", \",\", \".\").cast(FloatType())\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ud3uRcTge93",
      "metadata": {
        "id": "5ud3uRcTge93"
      },
      "outputs": [],
      "source": [
        "tiquets = tiquets.withColumn(\"dia\", date_format(col(\"fecha_operacion\"), \"dd-MM-yyyy\")) \\\n",
        "                 .withColumn(\"hora\", date_format(col(\"fecha_operacion\"), \"HH\"))\n",
        "\n",
        "tiquets = tiquets.withColumn(\"dia_semana\", date_format(col(\"fecha_operacion\"), \"EEEE\"))\n",
        "\n",
        "tiquets = tiquets.withColumn(\n",
        "    \"dia_semana\",\n",
        "    when(col(\"dia_semana\") == \"Monday\", 1)\n",
        "    .when(col(\"dia_semana\") == \"Tuesday\", 2)\n",
        "    .when(col(\"dia_semana\") == \"Wednesday\", 3)\n",
        "    .when(col(\"dia_semana\") == \"Thursday\", 4)\n",
        "    .when(col(\"dia_semana\") == \"Friday\", 5)\n",
        "    .when(col(\"dia_semana\") == \"Saturday\", 6)\n",
        "    .when(col(\"dia_semana\") == \"Sunday\", 7)\n",
        "    .otherwise(\"Desconocido\")\n",
        "    )\n",
        "\n",
        "tiquets = tiquets.withColumn(\"fin_de_semana\", when(col(\"dia_semana\").isin([6, 7]), 1).otherwise(0))\n",
        "\n",
        "tiquets = tiquets.withColumn(\"hora\",\n",
        "    when(col(\"hora\") == 0, 24).otherwise(col(\"hora\"))\n",
        ")\n",
        "\n",
        "tiquets = tiquets.withColumn(\"int_tiempo\", when((col(\"hora\") >= 6) & (col(\"hora\") < 12), \"Mañana\")\n",
        "                  .when((col(\"hora\") >= 12) & (col(\"hora\") < 19), \"Mediodia\")\n",
        "                  .when((col(\"hora\") >= 19) & (col(\"hora\") < 24), \"Tarde\")\n",
        "                  .otherwise(\"Noche\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PbqcbsfBhvUy",
      "metadata": {
        "id": "PbqcbsfBhvUy"
      },
      "outputs": [],
      "source": [
        "tiquets.select(\"matricula_parquimetro\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y3T8TYQ5iWZN",
      "metadata": {
        "id": "Y3T8TYQ5iWZN"
      },
      "outputs": [],
      "source": [
        "tiquets_agrupado = tiquets.groupBy(\"matricula_parquimetro\", \"int_tiempo\", \"fin_de_semana\").agg(\n",
        "    avg(\"minutos_tique\").alias(\"media_minutos\"),\n",
        "    avg(\"importe_tique\").alias(\"media_importe\"),\n",
        "    count(\"*\").alias(\"num_registros\")\n",
        ")\n",
        "\n",
        "tiquets_agrupado.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k1DO6UrOie91",
      "metadata": {
        "id": "k1DO6UrOie91"
      },
      "outputs": [],
      "source": [
        "tiquets_agrupado.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FauwrvQjioku",
      "metadata": {
        "id": "FauwrvQjioku"
      },
      "outputs": [],
      "source": [
        "tiquets_agrupado.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fOdJ8V83i1Mg",
      "metadata": {
        "id": "fOdJ8V83i1Mg"
      },
      "source": [
        "Volvemos a trabajar en pandas para unir la información relativa a los tiquets de cada parquímetros a la información de cada uno de ellos que teníamos anteriormente, para ello transformamos tiquets_agrupado a pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xVx_5aoojyAR",
      "metadata": {
        "id": "xVx_5aoojyAR"
      },
      "outputs": [],
      "source": [
        "tiquets_df = tiquets_agrupado.toPandas()\n",
        "\n",
        "tiquets_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "niV3R76LqYzA",
      "metadata": {
        "id": "niV3R76LqYzA"
      },
      "outputs": [],
      "source": [
        "duplicados = tiquets_df[tiquets_df.duplicated(keep=False)]\n",
        "duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Ean4paTl6Rj",
      "metadata": {
        "id": "3Ean4paTl6Rj"
      },
      "outputs": [],
      "source": [
        "tiquets_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VtNmDWY6l9M7",
      "metadata": {
        "id": "VtNmDWY6l9M7"
      },
      "outputs": [],
      "source": [
        "parquimetros.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20nqCzsZkL_8",
      "metadata": {
        "id": "20nqCzsZkL_8"
      },
      "outputs": [],
      "source": [
        "parkings_info = pd.merge(tiquets_df, parquimetros, left_on='matricula_parquimetro', right_on='matricula', how='inner')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rLYacTkonHbb",
      "metadata": {
        "id": "rLYacTkonHbb"
      },
      "source": [
        "No vamos a usar más ni la matrícula de los parquímetros ni el importe de los tiquets (nos da igual porque hay zonas verdes en los que no hay importe y estamos considerando aspectos generales de todas las zonas ser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_EAQwr_GlwAT",
      "metadata": {
        "id": "_EAQwr_GlwAT"
      },
      "outputs": [],
      "source": [
        "parkings_info = parkings_info[['int_tiempo', 'fin_de_semana', 'media_minutos', 'num_registros', 'cod_distrito', 'cod_barrio', 'calle','numero_finca','longitud','latitud']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I3jV1WiepP5k",
      "metadata": {
        "id": "I3jV1WiepP5k"
      },
      "outputs": [],
      "source": [
        "parkings_info.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IXyDdpcMi1Ir",
      "metadata": {
        "id": "IXyDdpcMi1Ir"
      },
      "source": [
        "Falta tráfico y el número de plazas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j0EeT86LpIVS",
      "metadata": {
        "id": "j0EeT86LpIVS"
      },
      "outputs": [],
      "source": [
        "parkings_info = pd.merge(parkings_info,\n",
        "                         calles_ser[['cod_barrio','calle','numero_finca','numero_plazas','color','bateria_linea','gis_x','gis_y']],\n",
        "                         on = ['cod_barrio','calle','numero_finca'],\n",
        "                         how = 'inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VfhRKi4vnlVm",
      "metadata": {
        "id": "VfhRKi4vnlVm"
      },
      "outputs": [],
      "source": [
        "parkings_info.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XGlWRJSoqhwH",
      "metadata": {
        "id": "XGlWRJSoqhwH"
      },
      "outputs": [],
      "source": [
        "parkings_info.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3SZqjlU9qj00",
      "metadata": {
        "id": "3SZqjlU9qj00"
      },
      "outputs": [],
      "source": [
        "duplicados = parkings_info[parkings_info.duplicated(keep=False)]\n",
        "duplicados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yxzAIvZVrsfp",
      "metadata": {
        "id": "yxzAIvZVrsfp"
      },
      "source": [
        "Vamos a leer únicamente datos de tráfico de enero a marzo del 2025, los correspondientes al primer trimestre para así tener consistencia con los datos de tiquets, que el archivo para datos más actuales está dañado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9RVVTwApahz",
      "metadata": {
        "id": "f9RVVTwApahz"
      },
      "outputs": [],
      "source": [
        "datos_enero = spark.read.csv(\"01-2025.csv\", header=True, inferSchema=True, sep=\";\")\n",
        "datos_febrero = spark.read.csv(\"02-2025.csv\", header=True, inferSchema=True, sep=\";\")\n",
        "datos_marzo = spark.read.csv(\"03-2025.csv\", header=True, inferSchema=True, sep=\";\")\n",
        "\n",
        "trafico = datos_enero.unionByName(datos_febrero).unionByName(datos_marzo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pF1U8RGjsCHs",
      "metadata": {
        "id": "pF1U8RGjsCHs"
      },
      "outputs": [],
      "source": [
        "trafico.show(5)\n",
        "trafico.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UV_e4AJxsExU",
      "metadata": {
        "id": "UV_e4AJxsExU"
      },
      "outputs": [],
      "source": [
        "trafico.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rV0qev0DsHfW",
      "metadata": {
        "id": "rV0qev0DsHfW"
      },
      "outputs": [],
      "source": [
        "trafico = trafico.withColumn(\"dia\", date_format(col(\"fecha\"), \"dd-MM-yyyy\")) \\\n",
        "                 .withColumn(\"hora\", date_format(col(\"fecha\"), \"HH\"))\n",
        "\n",
        "trafico = trafico.withColumn(\"dia_semana\", date_format(col(\"fecha\"), \"EEEE\"))\n",
        "\n",
        "trafico = trafico.withColumn(\n",
        "    \"dia_semana\",\n",
        "    when(col(\"dia_semana\") == \"Monday\", 1)\n",
        "    .when(col(\"dia_semana\") == \"Tuesday\", 2)\n",
        "    .when(col(\"dia_semana\") == \"Wednesday\", 3)\n",
        "    .when(col(\"dia_semana\") == \"Thursday\", 4)\n",
        "    .when(col(\"dia_semana\") == \"Friday\", 5)\n",
        "    .when(col(\"dia_semana\") == \"Saturday\", 6)\n",
        "    .when(col(\"dia_semana\") == \"Sunday\", 7)\n",
        "    .otherwise(\"Desconocido\")\n",
        "    )\n",
        "\n",
        "trafico = trafico.withColumn(\"fin_de_semana\", when(col(\"dia_semana\").isin([6, 7]), 1).otherwise(0))\n",
        "\n",
        "trafico = trafico.withColumn(\"hora\",\n",
        "    when(col(\"hora\") == 0, 24).otherwise(col(\"hora\"))\n",
        ")\n",
        "\n",
        "trafico = trafico.withColumn(\"int_tiempo\", when((col(\"hora\") >= 6) & (col(\"hora\") < 12), \"Mañana\")\n",
        "                  .when((col(\"hora\") >= 12) & (col(\"hora\") < 19), \"Mediodia\")\n",
        "                  .when((col(\"hora\") >= 19) & (col(\"hora\") < 24), \"Tarde\")\n",
        "                  .otherwise(\"Noche\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WK1hom0StYn-",
      "metadata": {
        "id": "WK1hom0StYn-"
      },
      "outputs": [],
      "source": [
        "trafico.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb-GhLUbtaj5",
      "metadata": {
        "id": "eb-GhLUbtaj5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "trafico_agrupado = trafico.groupBy(\"int_tiempo\",\"fin_de_semana\",\"id\").agg(\n",
        "    avg(\"intensidad\").alias(\"media_intensidad\"),\n",
        "    avg(\"ocupacion\").alias(\"media_ocupacion\"),\n",
        "    avg(\"carga\").alias(\"media_carga\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cl4rVy6nt6ak",
      "metadata": {
        "id": "Cl4rVy6nt6ak"
      },
      "outputs": [],
      "source": [
        "trafico_agrupado.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-fBekGWmt8JT",
      "metadata": {
        "id": "-fBekGWmt8JT"
      },
      "outputs": [],
      "source": [
        "trafico_agrupado.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "miD_na4FwAiH",
      "metadata": {
        "id": "miD_na4FwAiH"
      },
      "outputs": [],
      "source": [
        "trafico_df = trafico_agrupado.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GZlTCT18BsCa",
      "metadata": {
        "id": "GZlTCT18BsCa"
      },
      "outputs": [],
      "source": [
        "trafico_df['id'] = trafico_df['id'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FuifCI2VuV87",
      "metadata": {
        "id": "FuifCI2VuV87"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "ubicacion_pmed = pd.read_csv(\"pmed_ubicacion_06-2025.csv\",  encoding = 'latin-1', sep = ';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ItqOnCpBaKB",
      "metadata": {
        "id": "0ItqOnCpBaKB"
      },
      "outputs": [],
      "source": [
        "ubicacion_pmed.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vc8fXA_u-v57",
      "metadata": {
        "id": "vc8fXA_u-v57"
      },
      "outputs": [],
      "source": [
        "ubicacion_pmed['id'] = ubicacion_pmed['id'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r_Y2EH5-vx4W",
      "metadata": {
        "id": "r_Y2EH5-vx4W"
      },
      "outputs": [],
      "source": [
        "trafico_df = pd.merge(trafico_df,\n",
        "                      ubicacion_pmed[['id','longitud','latitud']],\n",
        "                      on='id',\n",
        "                      how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gGjySxpDv-TN",
      "metadata": {
        "id": "gGjySxpDv-TN"
      },
      "outputs": [],
      "source": [
        "trafico_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trafico_df.to_csv(\"trafico_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "fRHmTs7YJpWF"
      },
      "id": "fRHmTs7YJpWF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "QkOkrM4yjiWC"
      },
      "id": "QkOkrM4yjiWC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tratamiento espacial de los datos\n",
        "Tras haber leido los datos, a parte haciendo uso de spark, y haber hecho las modificaciones pertinentes, trabajamos ahora en google collab para el tratamiento espacial de los datos. Buscamos como finalidad llegar a dos dataset finales, uno con información de parquímetros y el otro sin información de parquímetros."
      ],
      "metadata": {
        "id": "l5B-oae2j3Lm"
      },
      "id": "l5B-oae2j3Lm"
    },
    {
      "cell_type": "code",
      "source": [
        "calles_ser = pd.read_csv(\"calles_ser_df.csv\")\n",
        "parkings_info = pd.read_csv(\"parking_info_df.csv\")\n",
        "trafico_df = pd.read_csv(\"trafico_df.csv\")"
      ],
      "metadata": {
        "id": "xXAsSuaUjVe2"
      },
      "id": "xXAsSuaUjVe2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trafico_df.info()"
      ],
      "metadata": {
        "id": "PJIEIOdvoWjc"
      },
      "id": "PJIEIOdvoWjc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que el dataframe trafico_df tiene valores nulos de longitud y latitud, los eliminamos"
      ],
      "metadata": {
        "id": "47iK8u1Womwh"
      },
      "id": "47iK8u1Womwh"
    },
    {
      "cell_type": "code",
      "source": [
        "trafico_df[trafico_df.isna().any(axis=1)]"
      ],
      "metadata": {
        "id": "HDBngZWvoF8e"
      },
      "id": "HDBngZWvoF8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trafico_df = trafico_df.dropna()"
      ],
      "metadata": {
        "id": "zYnpJ_MpouF4"
      },
      "id": "zYnpJ_MpouF4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8aDVZhtexxq5",
      "metadata": {
        "id": "8aDVZhtexxq5"
      },
      "source": [
        "Para unir el tráfico a la información de las zonas SER he decidido considerar geometría mediante geospanda. La componente espacial de los datos la tratamos mediante geometría"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k2VCpYTFw-48",
      "metadata": {
        "id": "k2VCpYTFw-48"
      },
      "outputs": [],
      "source": [
        "# Se añaden a los datasets una nueva columna a la que se llama geometria la cual contiene el punto (latitud, longitud)\n",
        "trafico_df['geometria'] = trafico_df.apply(lambda row: Point(row['longitud'], row['latitud']), axis=1)\n",
        "parkings_info['geometria'] = parkings_info.apply(lambda row: Point(row['longitud'], row['latitud']), axis=1)\n",
        "\n",
        "# Transformamos los dataframes a geodataframes haciendo uso de la columna geometria que se define arriba\n",
        "geo_trafico = gpd.GeoDataFrame(trafico_df, geometry='geometria', crs=\"EPSG:4326\")\n",
        "geo_parkings_info = gpd.GeoDataFrame(parkings_info, geometry='geometria', crs=\"EPSG:4326\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yg8Z0iyfyp4i",
      "metadata": {
        "id": "yg8Z0iyfyp4i"
      },
      "outputs": [],
      "source": [
        "# Se modifica el sistema de coordenadas a EPSG:25830 corresponde al sistema ETRS89 / UTM zone 30N, usado comúnmente en España.\n",
        "# Este sistema utiliza coordenadas planas en metros (en lugar de latitud/longitud en grados), lo cual es ideal para:\n",
        "# Calcular distancias con precisión.\n",
        "# Hacer uniones espaciales (spatial joins).\n",
        "# Trabajar con buffers y análisis espacial.\n",
        "geo_trafico = geo_trafico.to_crs(epsg=25830)\n",
        "geo_parkings_info = geo_parkings_info.to_crs(epsg=25830)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El interés de utilizar geodataframes es poder hacer join por cercanía, así para cada zona SER del dataframe, intervalo de tiempo y tipo de día se le asigna el tráfico correspondiente más cercano, para el mismo intervalo de tiempo y tipo de día. Esto se hace mediante la función sjoin_nearest:"
      ],
      "metadata": {
        "id": "duZXDTGEl-3r"
      },
      "id": "duZXDTGEl-3r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jTVcFJmQRANY",
      "metadata": {
        "id": "jTVcFJmQRANY"
      },
      "outputs": [],
      "source": [
        "# Lista para guardar resultados parciales\n",
        "resultados = []\n",
        "\n",
        "# Se obtienen grupos únicos combinados de las columnas clave\n",
        "grupos = geo_parkings_info[['fin_de_semana', 'int_tiempo']].drop_duplicates()\n",
        "\n",
        "for _, grupo in grupos.iterrows():\n",
        "    # Se filtran los dataframes que se van a unir según el subconjunto considerado\n",
        "    subset_parkings = geo_parkings_info[(geo_parkings_info['fin_de_semana'] == grupo['fin_de_semana']) & (geo_parkings_info['int_tiempo'] == grupo['int_tiempo'])]\n",
        "    subset_trafico = geo_trafico[(geo_trafico['fin_de_semana'] == grupo['fin_de_semana']) & (geo_trafico['int_tiempo'] == grupo['int_tiempo'])]\n",
        "\n",
        "    # Se hace sjoin_nearest para el grupo filtrado\n",
        "    join = gpd.sjoin_nearest(\n",
        "        subset_parkings[['geometria','int_tiempo','fin_de_semana','media_minutos','num_registros','numero_plazas','color','bateria_linea']],\n",
        "        subset_trafico[['geometria', 'media_intensidad', 'media_carga', 'media_ocupacion']],\n",
        "        how='left',\n",
        "        distance_col='distance'\n",
        "    )\n",
        "\n",
        "    # Se van almacenando los resultados\n",
        "    resultados.append(join)\n",
        "\n",
        "# Finalmente se concatenan todos los resultados\n",
        "geo_parking_info_completo = gpd.GeoDataFrame(pd.concat(resultados, ignore_index=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9vCYFYqkRBhV",
      "metadata": {
        "id": "9vCYFYqkRBhV"
      },
      "outputs": [],
      "source": [
        "# Se muestra el dataframe que contiene toda la información sobre parquímetros y tráfico según cercanía\n",
        "geo_parking_info_completo.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "izrJ6PgJylBy",
      "metadata": {
        "id": "izrJ6PgJylBy"
      },
      "outputs": [],
      "source": [
        "# Se eliminan las columas que no son de interes para el trabajo\n",
        "geo_parking_info_completo.drop(columns=['index_right', 'distance'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ADwt12Y7_jk",
      "metadata": {
        "id": "8ADwt12Y7_jk"
      },
      "outputs": [],
      "source": [
        "# Se comprueba si hay duplicados en el dataframe generado\n",
        "duplicados = geo_parking_info_completo[geo_parking_info_completo.duplicated(keep = False)]\n",
        "duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SqSVOEsj8Ze2",
      "metadata": {
        "id": "SqSVOEsj8Ze2"
      },
      "outputs": [],
      "source": [
        "# Se eliminan los duplicados\n",
        "geo_parking_info_completo.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LrD7RP3Hzj-g",
      "metadata": {
        "id": "LrD7RP3Hzj-g"
      },
      "source": [
        "El procedimiento para las zonas SER sobre las cuales no tenemos información acerca de parquímetros es el siguiente:\n",
        "\n",
        "\n",
        "1.   Se toman únicamente los registros de calles_ser_df que no estén en parkings_info (esto son las zonas de aparcamiento no informadas). Esto se lleva a cabo con un join left only.\n",
        "2.   Cada registro resultante se duplica de manera que cada calle ser aparezca para cada intervalo de tiempo y para cada tipo de dia. Esto se hace para que tenga consistencia con el dataframe informado.\n",
        "3. Se calculan las coordenadas a partir de gis_x y gis_y, se convierte en un geodataframe.\n",
        "4. Se hace join espacial por cercanía de la misma manera que se ha hecho en el caso anterior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wmGeIyl7yv7H",
      "metadata": {
        "id": "wmGeIyl7yv7H"
      },
      "outputs": [],
      "source": [
        "# Hacemos esto para simular un left_anti join, esto es una unión que conserve únicamente los valores de la tabla izquierda que no estén en la derecha\n",
        "aux = pd.merge(calles_ser, parkings_info[['gis_x','gis_y']], on=['gis_x', 'gis_y'], how='left', indicator=True)\n",
        "\n",
        "parkings_no_info = aux[aux['_merge'] == 'left_only'].drop(columns=['_merge'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1rD5FH2S0qTr",
      "metadata": {
        "id": "1rD5FH2S0qTr"
      },
      "outputs": [],
      "source": [
        "# De los 33.___ zonas ser totales, 28505 de ellas no están informadas\n",
        "parkings_no_info.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hXdv9nK21x-v",
      "metadata": {
        "id": "hXdv9nK21x-v"
      },
      "outputs": [],
      "source": [
        "# Se hace un producto cartesiando de los diferentes valores de fin_de_semana y de int_tiempo con el dataframe parkings_no_info\n",
        "fin_de_semana = [0, 1]\n",
        "dias_df = pd.DataFrame({'fin_de_semana': fin_de_semana})\n",
        "\n",
        "int_tiempo = ['Mañana', 'Mediodia', 'Tarde', 'Noche']\n",
        "horas_df = pd.DataFrame({'int_tiempo': int_tiempo})\n",
        "\n",
        "dias_horas_df = dias_df.merge(horas_df, how='cross')\n",
        "\n",
        "parkings_no_info_expandido = parkings_no_info.merge(dias_horas_df, how='cross')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x2nVqUgc2NmC",
      "metadata": {
        "id": "x2nVqUgc2NmC"
      },
      "outputs": [],
      "source": [
        "# Tenemos un registro para cada aparcamiento, para cada intervalo de tiempo y para cada tipo de día\n",
        "parkings_no_info_expandido.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CgTepsYs_V0_",
      "metadata": {
        "id": "CgTepsYs_V0_"
      },
      "outputs": [],
      "source": [
        "# Se crea la columan geometria como un punto (latitud, longitud)\n",
        "parkings_no_info_expandido['geometria'] = parkings_no_info_expandido.apply(lambda row: Point(row['longitud'], row['latitud']), axis=1)\n",
        "\n",
        "# Se convierte el dataframe en geodataframe\n",
        "geo_parkings_no_info = gpd.GeoDataFrame(parkings_no_info_expandido, geometry='geometria', crs=\"EPSG:4326\")\n",
        "\n",
        "# Lo pasamos a este sistema de coordenadas más apropiado para el tratamiento que vamos a realizar de los datos\n",
        "geo_parkings_no_info = geo_parkings_no_info.to_crs(epsg=25830)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fILPO759_s7j",
      "metadata": {
        "id": "fILPO759_s7j"
      },
      "outputs": [],
      "source": [
        "# De nuevo hacemos todo el procedimiento para hacer join por cercanía con el dataframe de tráfico, quizás lo mejor sea hacer una función y aplicarla dos veces\n",
        "resultados = []\n",
        "\n",
        "grupos = geo_parkings_no_info[['fin_de_semana', 'int_tiempo']].drop_duplicates()\n",
        "\n",
        "for _, grupo in grupos.iterrows():\n",
        "\n",
        "    subset_parkings = geo_parkings_no_info[(geo_parkings_no_info['fin_de_semana'] == grupo['fin_de_semana']) & (geo_parkings_no_info['int_tiempo'] == grupo['int_tiempo'])]\n",
        "    subset_trafico = geo_trafico[(geo_trafico['fin_de_semana'] == grupo['fin_de_semana']) & (geo_trafico['int_tiempo'] == grupo['int_tiempo'])]\n",
        "\n",
        "    join = gpd.sjoin_nearest(\n",
        "        subset_parkings[['geometria','int_tiempo','fin_de_semana','numero_plazas','color','bateria_linea']],\n",
        "        subset_trafico[['geometria', 'media_intensidad', 'media_carga', 'media_ocupacion']],\n",
        "        how='left',\n",
        "        distance_col='distance'\n",
        "    )\n",
        "\n",
        "    resultados.append(join)\n",
        "\n",
        "geo_parking_no_info_completo = gpd.GeoDataFrame(pd.concat(resultados, ignore_index=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OKGVXzfAALjQ",
      "metadata": {
        "id": "OKGVXzfAALjQ"
      },
      "outputs": [],
      "source": [
        "geo_parking_no_info_completo.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DNZdj0x7998b",
      "metadata": {
        "id": "DNZdj0x7998b"
      },
      "outputs": [],
      "source": [
        "# Se comprueba si hay duplicados\n",
        "duplicados = geo_parking_no_info_completo[geo_parking_no_info_completo.duplicated(keep = False)]\n",
        "duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I39mkQtJf-rL",
      "metadata": {
        "id": "I39mkQtJf-rL"
      },
      "outputs": [],
      "source": [
        "# Se eliminan las coordenadas que no nos interesan para el trabajo\n",
        "geo_parking_no_info_completo.drop(columns=['index_right', 'distance'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mG9II_7cX06c",
      "metadata": {
        "id": "mG9II_7cX06c"
      },
      "source": [
        "En este punto ya tenemos los dataframes que queríamos visualizar, se pueden visualizar los puntos de zonas ser con información conocida y aquellos de los que no tenemos información y a partir de estos continuaremos con un análisis exploratorio de ambos conjuntos y posteriormente con el modelado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DqbKWfhTbThr",
      "metadata": {
        "id": "DqbKWfhTbThr"
      },
      "outputs": [],
      "source": [
        "df_parking_info = pd.DataFrame(geo_parking_info_completo)\n",
        "df_parking_no_info = pd.DataFrame(geo_parking_no_info_completo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XXlMHdmPrRV0",
      "metadata": {
        "id": "XXlMHdmPrRV0"
      },
      "outputs": [],
      "source": [
        "calles_ser.to_csv(\"calles_ser_df.csv\", index=False)\n",
        "trafico_df.to_csv(\"trafico_df.csv\", index=False)\n",
        "df_parking_info.to_csv(\"df_parking_info.csv\", index=False)\n",
        "df_parking_no_info.to_csv(\"df_parking_no_info.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bQw9a2PNaA8T",
      "metadata": {
        "id": "bQw9a2PNaA8T"
      },
      "source": [
        "## Análisis Exploratorio de Datos\n",
        "En esta sección vamos a análizar en profundidad la información que nos dan nuestros datos, para ello vamos a estudiar los conjuntos de datos\n",
        "\n",
        "\n",
        "*   calles_ser\n",
        "*   geo_parking_info_completo\n",
        "*   geo_parking_no_info_completo\n",
        "*   trafico_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xikzn7XsdfhS",
      "metadata": {
        "id": "xikzn7XsdfhS"
      },
      "source": [
        "Aunque el dataset calles_ser no se va a utilizar por el mismo, por la naturaleza y la intención del trabajo se ha analizado para comprender y visualizar la distribución de las zonas SER del centro de la capital... Recordar incluir análisis univariante y bivariante."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fZ5_wVeLLa1D",
      "metadata": {
        "id": "fZ5_wVeLLa1D"
      },
      "source": [
        "Cabe mencionar que se podría llevar un análisis aún más detallado de los datos, puestos que éstos contienen una gran cantidad de información, pero se ha decidido llevar a cabo un análisis general pero que abarque todo lo necesario para no alargar excesivamente la extensión del trabajo. En particular, se ha intentado que sea una descripción visual que se entienda de manera fácil y directa."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IQeiP7vG9jfS",
      "metadata": {
        "id": "IQeiP7vG9jfS"
      },
      "source": [
        "En primer lugar comenzamos con el análisis exploratorio del dataset calles_ser, éste contiene información sobre las calles de Madrid centro en las que podemos encontrar servicio de estacionamiento regulado, con información adicional como el número de plazas asignado a cada zona, si se trata de un aparcamiento en línea o en batería o el tipo de zona de aparcamiento de la que se trata según su color: azul, verde, roja, naranja o de alta ..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calles SER"
      ],
      "metadata": {
        "id": "9SC_HXhpvUku"
      },
      "id": "9SC_HXhpvUku"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tema de plotly para todas las gráficas\n",
        "import plotly.io as pio\n",
        "\n",
        "pio.templates.default = \"simple_white\""
      ],
      "metadata": {
        "id": "0gMpbD3-xUqm"
      },
      "id": "0gMpbD3-xUqm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0hSdEtn-Rqx",
      "metadata": {
        "id": "a0hSdEtn-Rqx"
      },
      "outputs": [],
      "source": [
        "calles_ser.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mFpWiwX59isx",
      "metadata": {
        "id": "mFpWiwX59isx"
      },
      "outputs": [],
      "source": [
        "calles_ser.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222U4-cmAfuP",
      "metadata": {
        "id": "222U4-cmAfuP"
      },
      "source": [
        "Analizamos los barrios principales, con más plazas, las calles, la proporción en la que son bateria_linea, la proporción en los colores del aparcamiento y la distribución en el número de plazas. Quitar gis_x y gis_y y mostrar aquí el mapa. Este y el de tráfico los analizamos para entender el comportamiento de los fenómenos que estamos estudiando, sin embargo, los dos df que hemos construido para trabajar en la sección anterior son los que vamos a tratar de cara al modelaje de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANÁLISIS UNIVARIANTE"
      ],
      "metadata": {
        "id": "lxhIHjE2vZaB"
      },
      "id": "lxhIHjE2vZaB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distrito"
      ],
      "metadata": {
        "id": "d72u2x55viP5"
      },
      "id": "d72u2x55viP5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Nuestros datos contienen información de 13 distritos distintos\n",
        "len(calles_ser['distrito'].unique())"
      ],
      "metadata": {
        "id": "QuHgGR6wvutU"
      },
      "id": "QuHgGR6wvutU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distritos sobre los cuales tenemos información\n",
        "print(calles_ser['distrito'].unique())"
      ],
      "metadata": {
        "id": "2chA5tDhwLY4"
      },
      "id": "2chA5tDhwLY4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nº de zonas SER en cada distrito\n",
        "frecuencias = calles_ser['distrito'].value_counts().reset_index()\n",
        "frecuencias.columns = ['categoria', 'frecuencia']\n",
        "\n",
        "fig = px.bar(\n",
        "    frecuencias,\n",
        "    x='categoria',\n",
        "    y='frecuencia',\n",
        "    text='frecuencia',\n",
        "    title='Gráfico de barras distritos',\n",
        "    labels={'categoria': 'Distritos', 'frecuencia': 'Frecuencia'},\n",
        "    color='frecuencia'\n",
        ")\n",
        "\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(xaxis_tickangle=-45)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Pp244ssJvh2d"
      },
      "id": "Pp244ssJvh2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "-0t3Cc4I-kGn",
      "metadata": {
        "id": "-0t3Cc4I-kGn"
      },
      "source": [
        "BARRIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nuestros datos contienen información de 63 barrios distintos\n",
        "len(calles_ser['barrio'].unique())"
      ],
      "metadata": {
        "id": "btb1ZHMsyH02"
      },
      "execution_count": null,
      "outputs": [],
      "id": "btb1ZHMsyH02"
    },
    {
      "cell_type": "code",
      "source": [
        "# Barrios sobre los cuales tenemos información\n",
        "print(calles_ser['barrio'].unique())"
      ],
      "metadata": {
        "id": "yFrIP2AnyH03"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yFrIP2AnyH03"
    },
    {
      "cell_type": "code",
      "source": [
        "# Nº de zonas SER en cada barrio, mostramos los 20 barrios que más aparecen en nuestros daots\n",
        "frecuencias = calles_ser['barrio'].value_counts().head(20).reset_index()\n",
        "frecuencias.columns = ['categoria', 'frecuencia']\n",
        "\n",
        "fig = px.bar(\n",
        "    frecuencias,\n",
        "    x='categoria',\n",
        "    y='frecuencia',\n",
        "    text='frecuencia',\n",
        "    title='Gráfico de barras barrios',\n",
        "    labels={'categoria': 'Barrios', 'frecuencia': 'Frecuencia'},\n",
        "    color='frecuencia'\n",
        ")\n",
        "\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(xaxis_tickangle=-45)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "st4ebpCdyH03"
      },
      "execution_count": null,
      "outputs": [],
      "id": "st4ebpCdyH03"
    },
    {
      "cell_type": "markdown",
      "source": [
        "CALLES"
      ],
      "metadata": {
        "id": "0fHdr1MCzHKd"
      },
      "id": "0fHdr1MCzHKd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XmD7gPoH-Td_",
      "metadata": {
        "id": "XmD7gPoH-Td_"
      },
      "outputs": [],
      "source": [
        "# Nº de calles de las que tenemos información\n",
        "calles_ser['calle'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OBKMRooHCfQq",
      "metadata": {
        "id": "OBKMRooHCfQq"
      },
      "outputs": [],
      "source": [
        "# No vamos a mostrar todas las calles que aparecen pero sí las 20 calles que más aparecen en nuestros datos\n",
        "# Vemos que la calle que más aparece es el Paseo de la Castellana\n",
        "frecuencias = calles_ser['calle'].value_counts().head(20).reset_index()\n",
        "frecuencias.columns = ['categoria', 'frecuencia']\n",
        "\n",
        "fig = px.bar(\n",
        "    frecuencias,\n",
        "    x='categoria',\n",
        "    y='frecuencia',\n",
        "    text='frecuencia',\n",
        "    title='Distribución de categorías',\n",
        "    labels={'categoria': 'Categoría', 'frecuencia': 'Frecuencia'},\n",
        "    color='frecuencia'\n",
        ")\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(xaxis_tickangle=-45)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ilrAYYhuGeEW",
      "metadata": {
        "id": "ilrAYYhuGeEW"
      },
      "source": [
        "Las variables bateria_linea y color las vamos a estudiar mediante gráficos de sectores circulares puesto que ambas tienen entre 2 y 5 valores distintos, por lo que estas visualizaciones son ideales para ver su distribución en éstos casos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oQzu14Jw_n5j",
      "metadata": {
        "id": "oQzu14Jw_n5j"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Variable categórica que quieres visualizar\n",
        "columna = 'bateria_linea'  # por ejemplo\n",
        "\n",
        "# Agrupar y contar ocurrencias\n",
        "df_pie = calles_ser[columna].value_counts().reset_index()\n",
        "df_pie.columns = [columna, 'cuentas']\n",
        "\n",
        "# Crear gráfico\n",
        "fig = px.pie(\n",
        "    df_pie,\n",
        "    names=columna,\n",
        "    values='cuentas',\n",
        "    title=f'Distribución de {columna}',\n",
        "    color_discrete_sequence=px.colors.qualitative.Set2\n",
        ")\n",
        "\n",
        "fig.update_traces(textinfo='percent+label')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V2jxT_jXHALd",
      "metadata": {
        "id": "V2jxT_jXHALd"
      },
      "source": [
        "cambiar los colores de ésto para que vaya en consonancia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E1q31xQOG48N",
      "metadata": {
        "id": "E1q31xQOG48N"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Variable categórica que quieres visualizar\n",
        "columna = 'color'  # por ejemplo\n",
        "\n",
        "colores = {\n",
        "    'Azul': 'blue',\n",
        "    'Verde': 'green',\n",
        "    'Rojo': 'red',\n",
        "    'Naranja': 'orange',\n",
        "    'Alta Rotación': 'purple'\n",
        "}\n",
        "\n",
        "# Agrupar y contar ocurrencias\n",
        "df_pie = calles_ser[columna].value_counts().reset_index()\n",
        "df_pie.columns = [columna, 'cuentas']\n",
        "\n",
        "# Crear gráfico\n",
        "fig = px.pie(\n",
        "    df_pie,\n",
        "    names=columna,\n",
        "    values='cuentas',\n",
        "    color=columna,\n",
        "    title=f'Distribución de {columna}',\n",
        "    color_discrete_map=colores\n",
        ")\n",
        "\n",
        "fig.update_traces(textinfo='percent+label')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numero de plazas"
      ],
      "metadata": {
        "id": "b5hdnKmUzs7L"
      },
      "id": "b5hdnKmUzs7L"
    },
    {
      "cell_type": "code",
      "source": [
        "# Nº total de plazas 177622 plazas de aparcamiento\n",
        "print(calles_ser['numero_plazas'].sum())"
      ],
      "metadata": {
        "id": "HOeOh9dB29SQ"
      },
      "id": "HOeOh9dB29SQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calles_ser['numero_plazas'].describe()"
      ],
      "metadata": {
        "id": "wbmBJIpHzsha"
      },
      "id": "wbmBJIpHzsha",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VkGEcGl3HNq0",
      "metadata": {
        "id": "VkGEcGl3HNq0"
      },
      "outputs": [],
      "source": [
        "# Vemos la distribución de la variable de manera visual mediante su histograma y boxplot\n",
        "variable = 'numero_plazas'\n",
        "\n",
        "# Crear subplots\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=1,\n",
        "    shared_xaxes=True,\n",
        "    vertical_spacing=0.05,\n",
        "    row_heights=[0.25, 0.75]\n",
        ")\n",
        "\n",
        "# Boxplot\n",
        "fig.add_trace(\n",
        "    go.Box(x=calles_ser[variable], name='Boxplot', boxpoints='outliers'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Histograma\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=calles_ser[variable], nbinsx=30, name='Histograma'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=f'Distribución de \"{variable}\": Histograma + Boxplot',\n",
        "    showlegend=False,\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que la mayoría de zonas SER tienen entre 0 y 9 plazas de aparcamiento, aunque lo normal es que tengan hasta 29. El máximo número de plazas de una zona SER es 183."
      ],
      "metadata": {
        "id": "MOoC4skZ0T87"
      },
      "id": "MOoC4skZ0T87"
    },
    {
      "cell_type": "code",
      "source": [
        "df_plazas = (\n",
        "    calles_ser\n",
        "    .groupby(['distrito', 'barrio'], as_index=False)\n",
        "    .agg({'numero_plazas': 'sum'})\n",
        ")\n"
      ],
      "metadata": {
        "id": "IiHeUjf_3Khp"
      },
      "id": "IiHeUjf_3Khp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.treemap(\n",
        "    df_plazas,\n",
        "    path=['distrito', 'barrio'],\n",
        "    values='numero_plazas',\n",
        "    title='Número de plazas por distrito y barrio'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "8ZyxyWB53Vsk"
      },
      "id": "8ZyxyWB53Vsk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_calles = (\n",
        "    calles_ser\n",
        "    .sort_values('numero_plazas', ascending=False)\n",
        "    .drop_duplicates(subset=['distrito', 'barrio'])\n",
        "    [['distrito', 'barrio', 'calle', 'numero_plazas']]\n",
        "    .sort_values(['distrito', 'barrio'])  # opcional para orden visual\n",
        ")"
      ],
      "metadata": {
        "id": "Sbov7OB92Mvx"
      },
      "id": "Sbov7OB92Mvx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_calles['Etiqueta'] = df_top_calles['distrito'] + ' | ' + df_top_calles['barrio'] + ' | ' + df_top_calles['calle']\n",
        "\n",
        "fig = px.bar(\n",
        "    df_top_calles,\n",
        "    x='numero_plazas',\n",
        "    y='Etiqueta',\n",
        "    orientation='h',\n",
        "    title='Calle con más plazas por barrio y distrito',\n",
        "    labels={'numero_plazas': 'Número de Plazas', 'Etiqueta': 'Distrito | Barrio | Calle'}\n",
        ")\n",
        "\n",
        "fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "dOIPwczW2omg"
      },
      "id": "dOIPwczW2omg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos por  ejemplo que el paseo de la castellana era la calle que más aparecía en nuestro dataframe, sin embargo, es porque en la propia calle existen muchas zonas SER distintas, no porque sea la calle con el mayor número de plazas de aparcamiento."
      ],
      "metadata": {
        "id": "-e97Wbx649i0"
      },
      "id": "-e97Wbx649i0"
    },
    {
      "cell_type": "code",
      "source": [
        "colores = {\n",
        "    'Azul': 'blue',\n",
        "    'Verde': 'green',\n",
        "    'Rojo': 'red',\n",
        "    'Naranja': 'orange',\n",
        "    'Alta Rotación': 'purple'\n",
        "}\n",
        "\n",
        "df_color = (\n",
        "    calles_ser\n",
        "    .groupby('color', as_index=False)\n",
        "    .agg({'numero_plazas': 'sum'})\n",
        "    .sort_values('numero_plazas', ascending=False)\n",
        ")\n",
        "\n",
        "fig = px.pie(\n",
        "    df_color,\n",
        "    names='color',\n",
        "    color='color',\n",
        "    color_discrete_map=colores,\n",
        "    values='numero_plazas',\n",
        "    title='Distribución de plazas por color'\n",
        ")\n",
        "\n",
        "fig.update_traces(textinfo='percent+label')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "vaz-q5546YgN"
      },
      "id": "vaz-q5546YgN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "WnvOSpdvHG0n",
      "metadata": {
        "id": "WnvOSpdvHG0n"
      },
      "source": [
        "Finalmente estudiamos la distribución del número de plazas de las zonas SER¡"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NEhZkgAm_2tm",
      "metadata": {
        "id": "NEhZkgAm_2tm"
      },
      "source": [
        "Podemos observar que la mayoría de las zonas SER se encuentran entre 1 y 15 plazas como nos indica el boxplot aunque también cuenta con numerosos outliers, correspondientes a zonas de aparcamiento más amplias."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3voGQw4pIYHh",
      "metadata": {
        "id": "3voGQw4pIYHh"
      },
      "source": [
        "Finalemente observamos un mapa de Madrid con las correspondientes zonas SER dibujadas en función de su color y su tamaño en función del número de plazas para obtener así una apreciación visual de lo que estamos estudiando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QACoc4Z0IXM5",
      "metadata": {
        "id": "QACoc4Z0IXM5"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Colores por tipo de zona\n",
        "color_map = {\n",
        "    'Azul': 'blue',\n",
        "    'Verde': 'green',\n",
        "    'Alta Rotación': 'purple',\n",
        "    'Rojo': 'red',\n",
        "    'Naranja': 'orange'\n",
        "}\n",
        "\n",
        "# Crear figura base\n",
        "fig = go.Figure()\n",
        "\n",
        "# Añadir todos los puntos (sin distinción por batería/línea)\n",
        "fig.add_trace(go.Scattermapbox(\n",
        "    lat=calles_ser['latitud'],\n",
        "    lon=calles_ser['longitud'],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=3,  # Tamaño fijo para todos los puntos\n",
        "        color=calles_ser['color'].map(color_map),\n",
        "        symbol='circle'\n",
        "    ),\n",
        "    text=calles_ser['calle'],\n",
        "    hovertemplate='<b>%{text}</b><br>' +\n",
        "                  'Color: %{marker.color}<br>' +\n",
        "                  'Nº plazas: %{customdata}<extra></extra>',\n",
        "    customdata=calles_ser['numero_plazas'],  # Para mostrar nº plazas en hover\n",
        "    name='Zonas SER'\n",
        "))\n",
        "\n",
        "# Configuración del mapa\n",
        "fig.update_layout(\n",
        "    mapbox=dict(\n",
        "        style=\"carto-positron\",\n",
        "        center=dict(\n",
        "            lat=calles_ser['latitud'].mean(),\n",
        "            lon=calles_ser['longitud'].mean()\n",
        "        ),\n",
        "        zoom=11\n",
        "    ),\n",
        "    title='Mapa de zonas SER de Madrid centro',\n",
        "    height=700,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAFICO"
      ],
      "metadata": {
        "id": "xaecBp3kAFzM"
      },
      "id": "xaecBp3kAFzM"
    },
    {
      "cell_type": "markdown",
      "id": "63u6RYsLMNYX",
      "metadata": {
        "id": "63u6RYsLMNYX"
      },
      "source": [
        "Hacer un análisis de los datos de tráfico, y mencionar que luego estos dos conjuntos de datos se han dividido en la sección anterior dedicada al pretratamiento de los datos para el posterior modelado. Así a continuación se hará un análisis muy breve de los conjuntos de datos df_parking_info y df_parking_no_info, sobre todo de cara a que estén preparados de manera idónea para la parte de modelización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S1uT8TgBNzXT",
      "metadata": {
        "id": "S1uT8TgBNzXT"
      },
      "outputs": [],
      "source": [
        "trafico_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trafico_df['media_intensidad'].describe()"
      ],
      "metadata": {
        "id": "Ump7odPi9sln"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ump7odPi9sln"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VCOR05T9sln"
      },
      "outputs": [],
      "source": [
        "# Vemos la distribución de la variable de manera visual mediante su histograma y boxplot\n",
        "variable = 'media_intensidad'\n",
        "\n",
        "# Crear subplots\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=1,\n",
        "    shared_xaxes=True,\n",
        "    vertical_spacing=0.05,\n",
        "    row_heights=[0.25, 0.75]\n",
        ")\n",
        "\n",
        "# Boxplot\n",
        "fig.add_trace(\n",
        "    go.Box(x=trafico_df[variable], name='Boxplot', boxpoints='outliers'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Histograma\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=trafico_df[variable], nbinsx=30, name='Histograma'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=f'Distribución de \"{variable}\": Histograma + Boxplot',\n",
        "    showlegend=False,\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "id": "7VCOR05T9sln"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que es una distribución muy asimétrica hacia la izquierda"
      ],
      "metadata": {
        "id": "AEhMtZnm979I"
      },
      "id": "AEhMtZnm979I"
    },
    {
      "cell_type": "code",
      "source": [
        "trafico_df['media_ocupacion'].describe()"
      ],
      "metadata": {
        "id": "DAU1X1gN9qtt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DAU1X1gN9qtt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqpfumOE9qtu"
      },
      "outputs": [],
      "source": [
        "# Vemos la distribución de la variable de manera visual mediante su histograma y boxplot\n",
        "variable = 'media_ocupacion'\n",
        "\n",
        "# Crear subplots\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=1,\n",
        "    shared_xaxes=True,\n",
        "    vertical_spacing=0.05,\n",
        "    row_heights=[0.25, 0.75]\n",
        ")\n",
        "\n",
        "# Boxplot\n",
        "fig.add_trace(\n",
        "    go.Box(x=trafico_df[variable], name='Boxplot', boxpoints='outliers'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Histograma\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=trafico_df[variable], nbinsx=30, name='Histograma'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=f'Distribución de \"{variable}\": Histograma + Boxplot',\n",
        "    showlegend=False,\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "id": "ZqpfumOE9qtu"
    },
    {
      "cell_type": "code",
      "source": [
        "trafico_df['media_carga'].describe()"
      ],
      "metadata": {
        "id": "xKTWmiR6-NEc"
      },
      "id": "xKTWmiR6-NEc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vemos la distribución de la variable de manera visual mediante su histograma y boxplot\n",
        "variable = 'media_carga'\n",
        "\n",
        "# Crear subplots\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=1,\n",
        "    shared_xaxes=True,\n",
        "    vertical_spacing=0.05,\n",
        "    row_heights=[0.25, 0.75]\n",
        ")\n",
        "\n",
        "# Boxplot\n",
        "fig.add_trace(\n",
        "    go.Box(x=trafico_df[variable], name='Boxplot', boxpoints='outliers'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Histograma\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=trafico_df[variable], nbinsx=30, name='Histograma'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=f'Distribución de \"{variable}\": Histograma + Boxplot',\n",
        "    showlegend=False,\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "4gS3l8wD-M28"
      },
      "id": "4gS3l8wD-M28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thufgdcgM1Wn",
      "metadata": {
        "id": "thufgdcgM1Wn"
      },
      "outputs": [],
      "source": [
        "# Se calcula la matriz de correlación para las variables\n",
        "corr = trafico_df[['media_intensidad','media_ocupacion','media_carga']].corr()\n",
        "\n",
        "fig = px.imshow(\n",
        "    corr,\n",
        "    text_auto=True,\n",
        "    color_continuous_scale='OrRd',\n",
        "    title='Matriz de Correlación',\n",
        "    aspect='auto'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_wSEBSV-rra"
      },
      "outputs": [],
      "source": [
        "# Crear figura base\n",
        "fig = go.Figure()\n",
        "\n",
        "# Añadir todos los puntos (sin distinción por batería/línea)\n",
        "fig.add_trace(go.Scattermapbox(\n",
        "    lat=trafico_df['latitud'],\n",
        "    lon=trafico_df['longitud'],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=3,\n",
        "        color = 'black',\n",
        "        symbol='circle'\n",
        "    ),\n",
        "    name=''\n",
        "))\n",
        "\n",
        "# Configuración del mapa\n",
        "fig.update_layout(\n",
        "    mapbox=dict(\n",
        "        style=\"carto-positron\",\n",
        "        center=dict(\n",
        "            lat=trafico_df['latitud'].mean(),\n",
        "            lon=trafico_df['longitud'].mean()\n",
        "        ),\n",
        "        zoom=11\n",
        "    ),\n",
        "    title='Mapa de puntos de medida de tráfico de Madrid',\n",
        "    height=700,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "id": "K_wSEBSV-rra"
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.box(\n",
        "    trafico_df,\n",
        "    x='int_tiempo',\n",
        "    y='media_intensidad',\n",
        "    color='fin_de_semana',\n",
        "    title='Distribución de intensidad media por tramo horario y tipo de día',\n",
        "    labels={'tramo_tiempo': 'Tramo horario', 'ocupacion_media': 'Ocupación media'}\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "LB2AIHcL_gET"
      },
      "id": "LB2AIHcL_gET",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.box(\n",
        "    trafico_df,\n",
        "    x='int_tiempo',\n",
        "    y='media_ocupacion',\n",
        "    color='fin_de_semana',\n",
        "    title='Distribución de intensidad media por tramo horario y tipo de día',\n",
        "    labels={'tramo_tiempo': 'Tramo horario', 'ocupacion_media': 'Ocupación media'}\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "k2g8pyJQ_tPU"
      },
      "id": "k2g8pyJQ_tPU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "v4150v8HOOfP",
      "metadata": {
        "id": "v4150v8HOOfP"
      },
      "source": [
        "Vemos que sobretodo la variable media_carga guarda una correlación considerable con las dos restantes por lo que se podría considerar descartarla."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DF_PARKING_INFO"
      ],
      "metadata": {
        "id": "MajjJWMIARYX"
      },
      "id": "MajjJWMIARYX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2s_fuRhm3bds",
      "metadata": {
        "id": "2s_fuRhm3bds"
      },
      "outputs": [],
      "source": [
        "df_parking_info.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PM88goC7Z_cQ",
      "metadata": {
        "id": "PM88goC7Z_cQ"
      },
      "outputs": [],
      "source": [
        "df_parking_info.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_4j-dezKb8iO",
      "metadata": {
        "id": "_4j-dezKb8iO"
      },
      "source": [
        "Sobretodo interpretar las variables num_registros y media_minutos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HeRY81JNdR3l",
      "metadata": {
        "id": "HeRY81JNdR3l"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "columnas_numericas = ['media_minutos', 'num_registros', 'numero_plazas',\n",
        "                      'media_intensidad', 'media_carga', 'media_ocupacion']\n",
        "\n",
        "# Crear subplots: 2 filas x 3 columnas (ajusta según cuántas columnas tengas)\n",
        "fig = make_subplots(rows=2, cols=3, subplot_titles=columnas_numericas)\n",
        "\n",
        "# Añadir cada histograma\n",
        "for i, columna in enumerate(columnas_numericas):\n",
        "    row = i // 3 + 1\n",
        "    col = i % 3 + 1\n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=df_parking_info[columna], nbinsx=30, name=columna),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=600, width=1000, title_text=\"Histogramas de variables numéricas\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Arsp_T_kjnvL",
      "metadata": {
        "id": "Arsp_T_kjnvL"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Crear subplots: 2 filas x 3 columnas\n",
        "fig = make_subplots(rows=2, cols=3, subplot_titles=columnas_numericas)\n",
        "\n",
        "# Añadir cada boxplot\n",
        "for i, columna in enumerate(columnas_numericas):\n",
        "    row = i // 3 + 1\n",
        "    col = i % 3 + 1\n",
        "    fig.add_trace(\n",
        "        go.Box(y=df_parking_info[columna], name=columna),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=600, width=1000, title_text=\"Boxplots de variables numéricas\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7n76SEnf-pK_",
      "metadata": {
        "id": "7n76SEnf-pK_"
      },
      "source": [
        "Podemos ver que las distribuciones en general son muy asimétricas a la izquierda con largas colas, lo que nos informa de la abundante presencia de outliers. Que a continuación trataremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hv3qP5LTnqk6",
      "metadata": {
        "id": "Hv3qP5LTnqk6"
      },
      "outputs": [],
      "source": [
        "df_parking_info['color'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FsCkT-VGfnUm",
      "metadata": {
        "id": "FsCkT-VGfnUm"
      },
      "outputs": [],
      "source": [
        "df_parking_no_info.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xpgTmRVNfP6Y",
      "metadata": {
        "id": "xpgTmRVNfP6Y"
      },
      "outputs": [],
      "source": [
        "df_parking_no_info.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z3xDgPcq_NN8",
      "metadata": {
        "id": "Z3xDgPcq_NN8"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "columnas_numericas = ['numero_plazas', 'media_intensidad', 'media_carga', 'media_ocupacion']\n",
        "\n",
        "fig = make_subplots(rows=1, cols=4, subplot_titles=columnas_numericas)\n",
        "\n",
        "# Añadir cada histograma\n",
        "for i, columna in enumerate(columnas_numericas):\n",
        "    row = 1\n",
        "    col = i + 1\n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=df_parking_info[columna], nbinsx=30, name=columna),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=600, width=1000, title_text=\"Histogramas de variables numéricas\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ohNiqcAMcC",
      "metadata": {
        "id": "85ohNiqcAMcC"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "fig = make_subplots(rows=1, cols=4, subplot_titles=columnas_numericas)\n",
        "\n",
        "for i, columna in enumerate(columnas_numericas):\n",
        "    row = 1\n",
        "    col = i + 1\n",
        "    fig.add_trace(\n",
        "        go.Box(y=df_parking_info[columna], name=columna),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=600, width=1000, title_text=\"Boxplots de variables numéricas\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "geo_parkings_no_info = geo_parkings_no_info.to_crs(epsg=4326)\n",
        "geo_parkings_no_info['lon'] = geo_parkings_no_info.geometry.x\n",
        "geo_parkings_no_info['lat'] = geo_parkings_no_info.geometry.y\n",
        "\n",
        "geo_parkings_info = geo_parkings_info.to_crs(epsg=4326)\n",
        "geo_parkings_info['lon'] = geo_parkings_info.geometry.x\n",
        "geo_parkings_info['lat'] = geo_parkings_info.geometry.y\n"
      ],
      "metadata": {
        "id": "dlh5kRQICM8u"
      },
      "id": "dlh5kRQICM8u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# df1 en azul\n",
        "fig.add_trace(go.Scattermapbox(\n",
        "    lat=geo_parkings_no_info['lat'],\n",
        "    lon=geo_parkings_no_info['lon'],\n",
        "    mode='markers',\n",
        "    marker=dict(size=3, color='blue'),\n",
        "    name='Zonas SER sin info'\n",
        "))\n",
        "\n",
        "# df2 en rojo\n",
        "fig.add_trace(go.Scattermapbox(\n",
        "    lat=geo_parkings_info['lat'],\n",
        "    lon=geo_parkings_info['lon'],\n",
        "    mode='markers',\n",
        "    marker=dict(size=3, color='red'),\n",
        "    name='Zonas SER info'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    mapbox_zoom=12,\n",
        "    mapbox_center={\"lat\": geo_parkings_no_info['lat'].mean(), \"lon\": geo_parkings_no_info['lon'].mean()},\n",
        "    height=600,\n",
        "    title=\"Coordenadas de df1 y df2\"\n",
        ")\n",
        "\n",
        "fig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "MEVlsZy2FfNc"
      },
      "id": "MEVlsZy2FfNc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "y7HyzM5lA0K2",
      "metadata": {
        "id": "y7HyzM5lA0K2"
      },
      "source": [
        "Es interesante ver también la distribución de las variables numéricas por los tramos de tiempo que estamos considerando y si se trata de un día entre semana o un fin de semana."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "columnas_numericas = ['media_minutos', 'media_intensidad', 'media_carga', 'media_ocupacion','num_registros','numero_plazas']\n",
        "\n",
        "# Selección correcta de columnas\n",
        "grr = scatter_matrix(df_parking_info[columnas_numericas], figsize=(15, 15), diagonal='kde')\n",
        "plt.suptitle(\"Scatter Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6G9Cxz2zYMld"
      },
      "id": "6G9Cxz2zYMld",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HOzwRFAR33x4",
      "metadata": {
        "id": "HOzwRFAR33x4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que las relaciones no son lineales por lo que usamos métodos de regresión no lineales en nuestro problema."
      ],
      "metadata": {
        "id": "lzcBb0P8ZayI"
      },
      "id": "lzcBb0P8ZayI"
    },
    {
      "cell_type": "markdown",
      "id": "QgJ5FoBrLLA2",
      "metadata": {
        "id": "QgJ5FoBrLLA2"
      },
      "source": [
        "Tratamiento de outliers para los datos con los que vamos a entrenar el modelo de regresión"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l0IX6uW-artf",
      "metadata": {
        "id": "l0IX6uW-artf"
      },
      "source": [
        "## Elección de features\n",
        "\n",
        "Ésto en teoría hay que hacerlo antes del análisis exploratorio de los datos. Idea de feature engineering:\n",
        "\n",
        "* Ratio de ocupación por número de plazas: Ocupación de la carretera / Número de plazas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WxYRzIpsRwVj",
      "metadata": {
        "id": "WxYRzIpsRwVj"
      },
      "source": [
        "Hay que hacer encoding de las variables categóricas: bateria_linea, color, int_tiempo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing model performance:\n",
        "* Tuning the model parameters\n",
        "* Selecting a subset of features\n",
        "* Preprocessing the data"
      ],
      "metadata": {
        "id": "KgU3yR5MPDbt"
      },
      "id": "KgU3yR5MPDbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La carga depende de la intensidad y de la ocupación, luego este valor vamos a no considerarlo."
      ],
      "metadata": {
        "id": "GfxDuSrS-jKU"
      },
      "id": "GfxDuSrS-jKU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yEH2COlcRvxl",
      "metadata": {
        "id": "yEH2COlcRvxl"
      },
      "outputs": [],
      "source": [
        "df_parking_info.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a modificar las variables categóricas a numéricas, las codificamos:"
      ],
      "metadata": {
        "id": "4F6NXTmES1Va"
      },
      "id": "4F6NXTmES1Va"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evrP_sOBT1UK",
      "metadata": {
        "id": "evrP_sOBT1UK"
      },
      "outputs": [],
      "source": [
        "df_parking_info['bateria_linea'] = df_parking_info['bateria_linea'].replace({'Batería': 0, 'Línea': 1})\n",
        "df_parking_no_info['bateria_linea'] = df_parking_no_info['bateria_linea'].replace({'Batería': 0, 'Línea': 1})\n",
        "\n",
        "df_parking_info['color'] = df_parking_info['color'].replace({'Verde': 0, 'Azul': 1, 'Naranja': 2, 'Rojo': 3, 'Alta Rotación': 1})\n",
        "df_parking_no_info['color'] = df_parking_no_info['color'].replace({'Verde': 0, 'Azul': 1, 'Naranja': 2, 'Rojo': 3, 'Alta Rotación': 1})\n",
        "\n",
        "df_parking_info['int_tiempo'] = df_parking_info['int_tiempo'].replace({'Mañana': 0, 'Mediodia': 1, 'Tarde':2, 'Noche':3})\n",
        "df_parking_no_info['int_tiempo'] = df_parking_no_info['int_tiempo'].replace({'Mañana': 0, 'Mediodia': 1, 'Tarde':2, 'Noche':3})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos añadir nuevas features como: ratio_ocupacion_por_nplazas, en principio numero plazas no es cero entonces no habría problemas"
      ],
      "metadata": {
        "id": "nkhfF5JTTFFA"
      },
      "id": "nkhfF5JTTFFA"
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_info['ratio_ocupacion_por_nplazas'] = df_parking_info['media_ocupacion'] / df_parking_info['numero_plazas']\n",
        "df_parking_no_info['ratio_ocupacion_por_nplazas'] = df_parking_no_info['media_ocupacion'] / df_parking_no_info['numero_plazas']\n",
        "\n",
        "df_parking_info['ratio_intensidad_por_nplazas'] = df_parking_info['media_intensidad'] / df_parking_info['numero_plazas']\n",
        "df_parking_no_info['ratio_intensidad_por_nplazas'] = df_parking_no_info['media_intensidad'] / df_parking_no_info['numero_plazas']\n",
        "\n",
        "df_parking_info['ratio_carga_por_nplazas'] = df_parking_info['media_carga'] / df_parking_info['numero_plazas']\n",
        "df_parking_no_info['ratio_carga_por_nplazas'] = df_parking_no_info['media_carga'] / df_parking_no_info['numero_plazas']"
      ],
      "metadata": {
        "id": "5Mlf1iLKSvSU"
      },
      "id": "5Mlf1iLKSvSU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modificamos la variables que queremos estimar para hacerlas ver más como una distribución gaussiana y que su distribución no sea tan asimétrica."
      ],
      "metadata": {
        "id": "URYt7ZinU9gx"
      },
      "id": "URYt7ZinU9gx"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df_parking_info['num_registros_log'] = np.log(df_parking_info['num_registros'] + 1)\n",
        "df_parking_info['media_minutos_log'] = np.log(df_parking_info['media_minutos'] + 1)"
      ],
      "metadata": {
        "id": "Pf_wlQMl1f47"
      },
      "id": "Pf_wlQMl1f47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suponiendo que tus variables se llaman 'var1' y 'var2'\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Histograma de la primera variable\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df_parking_info['num_registros_log'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Histograma de num_registros_log')\n",
        "plt.xlabel('num_registros_log')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Histograma de la segunda variable\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df_parking_info['media_minutos_log'], bins=30, color='salmon', edgecolor='black')\n",
        "plt.title('Histograma de media_minutos_log')\n",
        "plt.xlabel('media_minutos_log')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GMiakgECVFRb"
      },
      "id": "GMiakgECVFRb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N8Zr8L1kWJLG"
      },
      "id": "N8Zr8L1kWJLG"
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_info['log_ratio_ocupacion_por_nplazas'] = np.log(df_parking_info['ratio_ocupacion_por_nplazas'] + 1)\n",
        "df_parking_info['log_ratio_intensidad_por_nplazas'] = np.log(df_parking_info['ratio_intensidad_por_nplazas'] + 1)\n",
        "df_parking_info['log_ratio_carga_por_nplazas'] = np.log(df_parking_info['ratio_carga_por_nplazas'] + 1)\n",
        "\n",
        "df_parking_no_info['log_ratio_ocupacion_por_nplazas'] = np.log(df_parking_no_info['ratio_ocupacion_por_nplazas'] + 1)\n",
        "df_parking_no_info['log_ratio_intensidad_por_nplazas'] = np.log(df_parking_no_info['ratio_intensidad_por_nplazas'] + 1)\n",
        "df_parking_no_info['log_ratio_carga_por_nplazas'] = np.log(df_parking_no_info['ratio_carga_por_nplazas'] + 1)"
      ],
      "metadata": {
        "id": "NqmUnGftVn16"
      },
      "id": "NqmUnGftVn16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suponiendo que tus variables se llaman 'var1' y 'var2'\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Histograma de la primera variable\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df_parking_info['log_ratio_ocupacion_por_nplazas'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Histograma de log_ratio_ocupacion_por_nplazas')\n",
        "plt.xlabel('log_ratio_ocupacion_por_nplazas')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Histograma de la segunda variable\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df_parking_info['log_ratio_intensidad_por_nplazas'], bins=30, color='salmon', edgecolor='black')\n",
        "plt.title('Histograma de log_ratio_intensidad_por_nplazas')\n",
        "plt.xlabel('log_ratio_intensidad_por_nplazas')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Histograma de la segunda variable\n",
        "# plt.subplot(1, 3, 3)\n",
        "# plt.hist(df_parking_info['log_ratio_carga_por_nplazas'], bins=30, color='salmon', edgecolor='black')\n",
        "# plt.title('Histograma de log_ratio_carga_por_nplazas')\n",
        "# plt.xlabel('log_ratio_carga_por_nplazas')\n",
        "# plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vcrf59IEV2EN"
      },
      "id": "Vcrf59IEV2EN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "jvhojE_Ia4Qk",
      "metadata": {
        "id": "jvhojE_Ia4Qk"
      },
      "source": [
        "## Modelo de regresión"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W5F-nVt2hf5j",
      "metadata": {
        "id": "W5F-nVt2hf5j"
      },
      "source": [
        "Voy a probar los modelos de Random Forest y XGBoost para el problema de regresión. Éste consiste en estimar el número de veces que se aparca en una determinada plaza de aparcamiento y la duración de la estancia a partir de la información que sí tenemos de los parquímetros."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RnqTtzboV7Xd",
      "metadata": {
        "id": "RnqTtzboV7Xd"
      },
      "source": [
        "[Data Preparation for XGBoost](https://https://xgboosting.com/data-preparation-for-xgboost/) , de aquí se saca la preparación de los datos para XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JG1qWotdWum9",
      "metadata": {
        "id": "JG1qWotdWum9"
      },
      "outputs": [],
      "source": [
        "df_parking_info.info()\n",
        "#df_parking_no_info.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CRwUtoB9W4iz",
      "metadata": {
        "id": "CRwUtoB9W4iz"
      },
      "outputs": [],
      "source": [
        "# Define the features (X) and the output labels (y)\n",
        "X = df_parking_info.drop(['geometria','media_minutos','num_registros','media_minutos_log','num_registros_log','ratio_ocupacion_por_nplazas','ratio_intensidad_por_nplazas','ratio_carga_por_nplazas','media_carga','log_ratio_carga_por_nplazas'], axis=1)\n",
        "y = df_parking_info[['media_minutos_log','num_registros_log']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vzkDYHKiXSX1",
      "metadata": {
        "id": "vzkDYHKiXSX1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "import xgboost as xgb\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0pp0RA7sX6Db",
      "metadata": {
        "id": "0pp0RA7sX6Db"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "xgb_base = xgb.XGBRegressor(random_state=42,\n",
        "    eval_metric='rmse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aGkXemh8Yf45",
      "metadata": {
        "id": "aGkXemh8Yf45"
      },
      "outputs": [],
      "source": [
        "def tune_regressor_hyperparameters(reg, param_grid, X_train, y_train, scoring='neg_mean_squared_error', n_splits=3):\n",
        "    '''\n",
        "    This function optimizes the hyperparameters for a regressor by searching over a specified hyperparameter grid.\n",
        "    It uses GridSearchCV and cross-validation (KFold) to evaluate different combinations of hyperparameters.\n",
        "    The combination with the highest negative mean squared error is selected as the default scoring metric.\n",
        "    The function returns the regressor with the optimal hyperparameters.\n",
        "    '''\n",
        "\n",
        "    # Create the cross-validation object using KFold\n",
        "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
        "\n",
        "    # Create the GridSearchCV object\n",
        "    reg_grid = GridSearchCV(reg, param_grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "    # Fit the GridSearchCV object to the training data\n",
        "    reg_grid.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best hyperparameters\n",
        "    best_hyperparameters = reg_grid.best_params_\n",
        "\n",
        "    # Return best_estimator_ attribute which gives us the best model that has been fitted to the training data\n",
        "    return reg_grid.best_estimator_, best_hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7qlghwFsYu4E",
      "metadata": {
        "id": "7qlghwFsYu4E"
      },
      "outputs": [],
      "source": [
        "xgb_param_grid = {\n",
        "    'max_depth': [3, 4],\n",
        "    'learning_rate': [0.01, 0.05],\n",
        "    'n_estimators': [200, 300],\n",
        "    'min_child_weight': [5, 7]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CnYvwbneYwZ0",
      "metadata": {
        "id": "CnYvwbneYwZ0"
      },
      "outputs": [],
      "source": [
        "# Tune the hyperparameters\n",
        "best_xgb, best_xgb_hyperparameters = tune_regressor_hyperparameters(xgb_base, xgb_param_grid, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hRcmdn6AYxne",
      "metadata": {
        "id": "hRcmdn6AYxne"
      },
      "outputs": [],
      "source": [
        "print('XGBoost Regressor Optimal Hyperparameters: \\n', best_xgb_hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yTDRt81IZB_N",
      "metadata": {
        "id": "yTDRt81IZB_N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name,cv=None):\n",
        "\n",
        "    # Predict on training and testing data\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for training data\n",
        "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "    rmse_train = np.sqrt(mse_train)\n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "    # Calculate metrics for testing data\n",
        "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "    rmse_test = np.sqrt(mse_test)\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    # Create a DataFrame for metrics\n",
        "    metrics_df = pd.DataFrame(data = [mae_test, mse_test, rmse_test, r2_test],\n",
        "                              index = ['MAE', 'MSE', 'RMSE', 'R2 Score'],\n",
        "                              columns = [model_name])\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"{model_name} Training Data Metrics:\")\n",
        "    print(\"MAE: {:.4f}\".format(mae_train))\n",
        "    print(\"MSE: {:.4f}\".format(mse_train))\n",
        "    print(\"RMSE: {:.4f}\".format(rmse_train))\n",
        "    print(\"R2 Score: {:.4f}\".format(r2_train))\n",
        "\n",
        "    print(f\"\\n{model_name} Testing Data Metrics:\")\n",
        "    print(\"MAE: {:.4f}\".format(mae_test))\n",
        "    print(\"MSE: {:.4f}\".format(mse_test))\n",
        "    print(\"RMSE: {:.4f}\".format(rmse_test))\n",
        "    print(\"R2 Score: {:.4f}\".format(r2_test))\n",
        "\n",
        "    # Optional cross-validation\n",
        "    if cv:\n",
        "        print(f\"\\n{model_name} Cross-Validation ({cv}-fold) Metrics:\")\n",
        "        scoring = {\n",
        "            'MAE': 'neg_mean_absolute_error',\n",
        "            'MSE': 'neg_mean_squared_error',\n",
        "            'R2': 'r2'\n",
        "        }\n",
        "\n",
        "        cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring=scoring)\n",
        "\n",
        "        print(\"Avg MAE (CV): {:.4f}\".format(-cv_results['test_MAE'].mean()))\n",
        "        print(\"Avg MSE (CV): {:.4f}\".format(-cv_results['test_MSE'].mean()))\n",
        "        print(\"Avg RMSE (CV): {:.4f}\".format(np.sqrt(-cv_results['test_MSE'].mean())))\n",
        "        print(\"Avg R2 (CV): {:.4f}\".format(cv_results['test_R2'].mean()))\n",
        "\n",
        "\n",
        "    return metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s-nZpeopZGyb",
      "metadata": {
        "id": "s-nZpeopZGyb"
      },
      "outputs": [],
      "source": [
        "xgb_result = evaluate_model(best_xgb, X_train, y_train, X_test, y_test, 'XGBoost',5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "df_parking_no_info_xgb = df_parking_no_info.copy()\n",
        "\n",
        "predictions = best_xgb.predict(df_parking_no_info_xgb.drop(['geometria','ratio_ocupacion_por_nplazas', 'ratio_intensidad_por_nplazas','distance','index_right'], axis = 1))\n",
        "\n",
        "# Transform predictions back to original scale\n",
        "num_registros = np.expm1(predictions[:, 1])\n",
        "media_minutos = np.expm1(predictions[:, 0])\n",
        "\n",
        "# Add to original dataframe\n",
        "df_parking_no_info_xgb['num_registros'] = num_registros\n",
        "df_parking_no_info_xgb['media_minutos'] = media_minutos\n",
        "\n",
        "# Display results\n",
        "df_parking_no_info_xgb[['num_registros', 'media_minutos']]"
      ],
      "metadata": {
        "id": "opc1T3Xhy-fb"
      },
      "id": "opc1T3Xhy-fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_no_info.describe()"
      ],
      "metadata": {
        "id": "PyzBfgN9231b"
      },
      "id": "PyzBfgN9231b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Q19NrAcWTvOz",
      "metadata": {
        "id": "Q19NrAcWTvOz"
      },
      "source": [
        "Hacer el tratamiento de outliers después del de regresión para el modelo de clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KYO_JTESsEWS",
      "metadata": {
        "id": "KYO_JTESsEWS"
      },
      "source": [
        "### Random Forest\n",
        "Decir por qué he elegido estos dos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TbCN6giXlRe7",
      "metadata": {
        "id": "TbCN6giXlRe7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "\n",
        "# Define the features (X) and the output labels (y)\n",
        "X = df_parking_info.drop(['geometria','media_minutos','num_registros','media_minutos_log','num_registros_log','ratio_ocupacion_por_nplazas','ratio_intensidad_por_nplazas'], axis=1)\n",
        "y = df_parking_info[['media_minutos_log','num_registros_log']]\n",
        "# Splitting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "u2Y5ggnJQAMD"
      },
      "id": "u2Y5ggnJQAMD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor(random_state=42)"
      ],
      "metadata": {
        "id": "plBDibxWOLUQ"
      },
      "id": "plBDibxWOLUQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load and prepare your data\n",
        "# X = your features, y = your target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [200],\n",
        "    'max_depth': [10],\n",
        "    'min_samples_split': [5, 10],\n",
        "    'min_samples_leaf': [2, 4],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "# 3. Initialize Random Forest\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "# 4. Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. Fit on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV score:\", -grid_search.best_score_)"
      ],
      "metadata": {
        "id": "SQojGOR6OO9O"
      },
      "id": "SQojGOR6OO9O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train the best model on full training set\n",
        "best_rf.fit(X_train, y_train)\n",
        "\n",
        "# 8. Make predictions\n",
        "train_predictions = best_rf.predict(X_train)\n",
        "test_predictions = best_rf.predict(X_test)\n",
        "\n",
        "# 9. Evaluate performance\n",
        "def evaluate_model(y_true, y_pred, dataset_name):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n{dataset_name} Performance:\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
        "\n",
        "# Evaluate on both sets\n",
        "train_metrics = evaluate_model(y_train, train_predictions, \"Training\")\n",
        "test_metrics = evaluate_model(y_test, test_predictions, \"Test\")"
      ],
      "metadata": {
        "id": "fvcqCLaBOWp4"
      },
      "id": "fvcqCLaBOWp4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "df_parking_no_info_rf = df_parking_no_info.copy()\n",
        "\n",
        "predictions = best_rf.predict(df_parking_no_info_rf.drop(['geometria','ratio_ocupacion_por_nplazas', 'ratio_intensidad_por_nplazas'], axis = 1))\n",
        "\n",
        "# Transform predictions back to original scale\n",
        "num_registros = np.expm1(predictions[:, 1])\n",
        "media_minutos = np.expm1(predictions[:, 0])\n",
        "\n",
        "# Add to original dataframe\n",
        "df_parking_no_info_rf['num_registros'] = num_registros\n",
        "df_parking_no_info_rf['media_minutos'] = media_minutos\n",
        "\n",
        "# Display results\n",
        "df_parking_no_info_rf[['num_registros', 'media_minutos']]"
      ],
      "metadata": {
        "id": "CSRQBBCPOdOF"
      },
      "id": "CSRQBBCPOdOF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xC4ju-m6lyOr",
      "metadata": {
        "id": "xC4ju-m6lyOr"
      },
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'XGBoost': [xgb_result.loc['MAE'].values[0],\n",
        "                xgb_result.loc['MSE'].values[0],\n",
        "                xgb_result.loc['RMSE'].values[0],\n",
        "                xgb_result.loc['R2 Score'].values[0]],\n",
        "    'Random Forest': [rf_result.loc['MAE'].values[0],\n",
        "                      rf_result.loc['MSE'].values[0],\n",
        "                      rf_result.loc['RMSE'].values[0],\n",
        "                      rf_result.loc['R2 Score'].values[0]]\n",
        "}, index=['MAE', 'MSE', 'RMSE', 'R2 Score'])\n",
        "\n",
        "print(\"\\nModel Comparison (Test Set):\")\n",
        "print(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7PBZwTZna62Y",
      "metadata": {
        "id": "7PBZwTZna62Y"
      },
      "source": [
        "## Modelo de clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_info"
      ],
      "metadata": {
        "id": "oOyw1tt8z5f9"
      },
      "id": "oOyw1tt8z5f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w_1nOGD_a2NK",
      "metadata": {
        "id": "w_1nOGD_a2NK"
      },
      "outputs": [],
      "source": [
        "df_parking_info.drop(columns=['num_registros_log', 'media_minutos_log'], inplace=True)\n",
        "\n",
        "df_parking = pd.concat([df_parking_info, df_parking_no_info])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking.head()"
      ],
      "metadata": {
        "id": "2a5HZBKtncLA"
      },
      "id": "2a5HZBKtncLA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking.info()"
      ],
      "metadata": {
        "id": "TIA5utDtWUN6"
      },
      "id": "TIA5utDtWUN6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TV2Lv9dzObgu",
      "metadata": {
        "id": "TV2Lv9dzObgu"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Initializing the IsolationForest model with a contamination parameter of 0.05\n",
        "model = IsolationForest(contamination=0.05, random_state=0)\n",
        "\n",
        "# Fitting the model on our dataset (converting DataFrame to NumPy to avoid warning)\n",
        "df_parking['Outlier_Scores'] = model.fit_predict(df_parking.iloc[:, 1:].to_numpy())\n",
        "\n",
        "# Creating a new column to identify outliers (1 for inliers and -1 for outliers)\n",
        "df_parking['Is_Outlier'] = [1 if x == -1 else 0 for x in df_parking['Outlier_Scores']]\n",
        "\n",
        "# Display the first few rows of the customer_data dataframe\n",
        "df_parking.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of inliers and outliers\n",
        "outlier_percentage = df_parking['Is_Outlier'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Plotting the percentage of inliers and outliers\n",
        "plt.figure(figsize=(12, 4))\n",
        "outlier_percentage.plot(kind='barh', color='#ff6200')\n",
        "\n",
        "# Adding the percentage labels on the bars\n",
        "for index, value in enumerate(outlier_percentage):\n",
        "    plt.text(value, index, f'{value:.2f}%', fontsize=15)\n",
        "\n",
        "plt.title('Percentage of Inliers and Outliers')\n",
        "plt.xticks(ticks=np.arange(0, 115, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "plt.ylabel('Is Outlier')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X0J8_TPZ5tIt"
      },
      "id": "X0J8_TPZ5tIt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the outliers for analysis\n",
        "outliers_data = df_parking[df_parking['Is_Outlier'] == 1]\n",
        "\n",
        "# Remove the outliers from the main dataset\n",
        "df_parking_cleaned = df_parking[df_parking['Is_Outlier'] == 0]\n",
        "\n",
        "# Drop the 'Outlier_Scores' and 'Is_Outlier' columns\n",
        "df_parking_cleaned = df_parking_cleaned.drop(columns=['Outlier_Scores', 'Is_Outlier'])\n",
        "\n",
        "# Reset the index of the cleaned data\n",
        "df_parking_cleaned.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "NU_fzda16f1j"
      },
      "id": "NU_fzda16f1j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "\n",
        "df_parking = df_parking.drop(columns=['Outlier_Scores', 'Is_Outlier'])\n",
        "\n",
        "# List of columns that don't need to be scaled\n",
        "columns_to_exclude = ['int_tiempo', 'fin_de_semana', 'color','bateria_linea','geometria']\n",
        "\n",
        "# List of columns that need to be scaled\n",
        "columns_to_scale = df_parking.columns.difference(columns_to_exclude)\n",
        "\n",
        "# Copy the cleaned dataset\n",
        "df_parking_scaled = df_parking.copy()\n",
        "\n",
        "# Applying the scaler to the necessary columns in the dataset\n",
        "df_parking_scaled[columns_to_scale] = scaler.fit_transform(df_parking_scaled[columns_to_scale])\n",
        "\n",
        "# Display the first few rows of the scaled data\n",
        "df_parking_scaled.head()"
      ],
      "metadata": {
        "id": "LPXsKoa961ef"
      },
      "id": "LPXsKoa961ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_scaled.info()"
      ],
      "metadata": {
        "id": "Kx6ky5YTn_Jv"
      },
      "id": "Kx6ky5YTn_Jv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsrOedkt3cwP"
      },
      "id": "dsrOedkt3cwP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_1_ok.info()"
      ],
      "metadata": {
        "id": "HkTWzCxJVlGk"
      },
      "id": "HkTWzCxJVlGk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"int_tiempo únicos:\", df_parking_scaled['int_tiempo'].unique())\n",
        "print(\"fin_de_semana únicos:\", df_parking_scaled['fin_de_semana'].unique())\n"
      ],
      "metadata": {
        "id": "gLwjG46NjBdp"
      },
      "id": "gLwjG46NjBdp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Configuración de estilos\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({'font.size': 10})\n",
        "\n",
        "# Mapeo de tiempo a etiquetas legibles\n",
        "tiempo_labels = {0: \"Mañana\", 1: \"Mediodía\", 2: \"Tarde\", 3: \"Noche\"}\n",
        "int_tiempos = list(tiempo_labels.keys())\n",
        "fin_de_semana_vals = [0, 1]\n",
        "\n",
        "# Crear figura con subgráficos\n",
        "fig, axes = plt.subplots(len(int_tiempos), len(fin_de_semana_vals), figsize=(12, 16), sharey=True)\n",
        "\n",
        "for i, tiempo in enumerate(int_tiempos):\n",
        "    for j, fin_sem in enumerate(fin_de_semana_vals):\n",
        "        ax = axes[i, j]\n",
        "        df_subset = df_parking_scaled[\n",
        "            (df_parking_scaled['int_tiempo'] == tiempo) &\n",
        "            (df_parking_scaled['fin_de_semana'] == fin_sem)\n",
        "        ]\n",
        "\n",
        "        if not df_subset.empty:\n",
        "            pca = PCA()\n",
        "            numeric_data = df_subset.select_dtypes(include=[np.number])\n",
        "            pca.fit(numeric_data)\n",
        "            evr = pca.explained_variance_ratio_\n",
        "\n",
        "            sns.barplot(x=list(range(1, len(evr) + 1)), y=evr, color='#fcb768', ax=ax)\n",
        "            ax.set_title(f\"{tiempo_labels[tiempo]} - Fin de semana: {'Sí' if fin_sem else 'No'}\")\n",
        "            ax.set_xlabel(\"Componente principal\")\n",
        "            ax.set_ylabel(\"Varianza explicada\")\n",
        "\n",
        "            for k, val in enumerate(evr):\n",
        "                ax.text(k, val + 0.01, f\"{val:.2f}\", ha='center', va='bottom', fontsize=8)\n",
        "        else:\n",
        "            ax.set_title(f\"{tiempo_labels[tiempo]} - Fin de semana: {'Sí' if fin_sem else 'No'}\")\n",
        "            ax.text(0.5, 0.5, 'Sin datos', transform=ax.transAxes, ha='center', va='center', fontsize=10)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "\n",
        "# Ajustes finales\n",
        "plt.suptitle(\"Varianza explicada por PCA en cada franja horaria y tipo de día\", fontsize=16, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i_I8TzHz7Lzk"
      },
      "id": "i_I8TzHz7Lzk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para subconjunto de datos que tomemos podemos elegir 4 componentes principales. Buscamos automatizar el proceso."
      ],
      "metadata": {
        "id": "Ypq0nhuLjlD2"
      },
      "id": "Ypq0nhuLjlD2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puedo hacer para cada subconjunto estudiar el clustering individualmente y luego juntarlo todo en un único df."
      ],
      "metadata": {
        "id": "7qKFcLOWv60J"
      },
      "id": "7qKFcLOWv60J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering para un día entre semana por la mañana\n",
        "Estudiamos el subconjunto formado por los registros que corresponden los datos de los días entre semana por la mañana. El estudio para este subconjunto se realiza de manera muy detallada mediante que los demás subconjuntos se estudian de manera más general, aunque con el detalle suficiente para llevar a cabo el modelo de manera efectiva. Subconjunto_aux es el df que se utilizará para entrenar el modelo."
      ],
      "metadata": {
        "id": "J_Li_UK3kqfv"
      },
      "id": "J_Li_UK3kqfv"
    },
    {
      "cell_type": "code",
      "source": [
        "subconjunto = df_parking_scaled[(df_parking_scaled['int_tiempo'] == 0) & (df_parking_scaled['fin_de_semana'] == 0)]\n",
        "subconjunto_aux = subconjunto[['media_minutos', 'num_registros', 'numero_plazas','color','bateria_linea','log_ratio_intensidad_por_nplazas','media_ocupacion']]"
      ],
      "metadata": {
        "id": "30qce11tkzKH"
      },
      "id": "30qce11tkzKH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este hacer un estudio del Principal Component Analysis más detallado, para los otros ya no."
      ],
      "metadata": {
        "id": "nLQwvshJ6pKb"
      },
      "id": "nLQwvshJ6pKb"
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=4)\n",
        "\n",
        "df_parking_pca = pca.fit_transform(subconjunto_aux)\n",
        "\n",
        "# Nombramos la columnas según la componente principal a la que correspondan\n",
        "df_parking_pca = pd.DataFrame(df_parking_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])"
      ],
      "metadata": {
        "id": "P1cuWnBR7nyz"
      },
      "id": "P1cuWnBR7nyz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estudiamos tanto el método del codo como el silhouette score para estimar el valor de k antes de llevar a cabo el algoritmo de kmeans final."
      ],
      "metadata": {
        "id": "D_95B6RR7UgR"
      },
      "id": "D_95B6RR7UgR"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "X = df_parking_pca.select_dtypes(include=[np.number])\n",
        "\n",
        "valores_k = range(2, 8)\n",
        "inercias = []\n",
        "silhouettes_scores = []\n",
        "\n",
        "for k in valores_k:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    inercias.append(kmeans.inertia_)\n",
        "    silhouettes_scores.append(silhouette_score(X, labels))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axs[0].plot(valores_k, inercias, marker='o', color='darkorange')\n",
        "axs[0].set_title('Método del codo (Elbow)')\n",
        "axs[0].set_xlabel('Número de clusters (k)')\n",
        "axs[0].set_ylabel('Inercia')\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].plot(valores_k, silhouettes_scores, marker='s', linestyle='--', color='teal')\n",
        "axs[1].set_title('Silhouette Score por número de clusters')\n",
        "axs[1].set_xlabel('Número de clusters (k)')\n",
        "axs[1].set_ylabel('Silhouette Score')\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "07WgQylP8Z1X"
      },
      "id": "07WgQylP8Z1X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos observar que tenemos valores relativamente altos del silhouette score para valores altos de inercia, esto nos indica que la distinción entre clusters es buena pero los clusters son amplios, no compactos, y que los puntos tienen alta dispersión. Para datos reales como los que estamos usando en este trabajo un silhouette score de más de 0.45 lo vamos a considerar como aceptable, puesto que los datos reales tienen ruido que complica la segmentación de éstos. En éste caso"
      ],
      "metadata": {
        "id": "k8W_Ovko81hq"
      },
      "id": "k8W_Ovko81hq"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "\n",
        "# Apply KMeans clustering using the optimal k\n",
        "kmeans = KMeans(n_clusters=2, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "kmeans.fit(df_parking_pca)\n",
        "\n",
        "# Get the frequency of each cluster\n",
        "cluster_frequencies = Counter(kmeans.labels_)\n",
        "\n",
        "# Append the new cluster labels back to the original dataset\n",
        "df_parking_1['cluster'] = kmeans.labels_\n",
        "# Append the new cluster labels to the PCA version of the dataset\n",
        "df_parking_pca['cluster'] = kmeans.labels_\n",
        "\n"
      ],
      "metadata": {
        "id": "-iBkhhpC8nnb"
      },
      "id": "-iBkhhpC8nnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each cluster\n",
        "cluster_percentage = (df_parking_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h')\n",
        "\n",
        "# Adding percentages on the bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lFmUBlKm12Nu"
      },
      "id": "lFmUBlKm12Nu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histograms for each feature segmented by the clusters\n",
        "features = df_parking_1.columns[1:-1]\n",
        "clusters = df_parking_1['cluster'].unique()\n",
        "clusters.sort()\n",
        "\n",
        "# Setting up the subplots\n",
        "n_rows = len(features)\n",
        "n_cols = len(clusters)\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3*n_rows))\n",
        "\n",
        "# Plotting histograms\n",
        "for i, feature in enumerate(features):\n",
        "    for j, cluster in enumerate(clusters):\n",
        "        data = df_parking_1[df_parking_1['cluster'] == cluster][feature]\n",
        "        axes[i, j].hist(data, bins=20, edgecolor='w', alpha=0.7)\n",
        "        axes[i, j].set_title(f'Cluster {cluster} - {feature}', fontsize=15)\n",
        "        axes[i, j].set_xlabel('')\n",
        "        axes[i, j].set_ylabel('')\n",
        "\n",
        "# Adjusting layout to prevent overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uvF-NXhr2Q1q"
      },
      "id": "uvF-NXhr2Q1q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats_por_cluster = df_parking_1.drop('geometria', axis=1).groupby('cluster').mean()\n",
        "\n",
        "stats_por_cluster"
      ],
      "metadata": {
        "id": "aFzWjtUW5D7h"
      },
      "id": "aFzWjtUW5D7h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "# 1. Convertir columna de geometría (WKT -> Point)\n",
        "# df_parking_1['geometria'] = df_parking_1['geometria'].apply(wkt.loads)\n",
        "\n",
        "# 2. Crear GeoDataFrame con CRS EPSG:25830\n",
        "gdf = gpd.GeoDataFrame(df_parking_1, geometry='geometria', crs='EPSG:25830')\n",
        "\n",
        "# 3. Reproyectar a EPSG:4326 (lat/lon)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# 4. Extraer latitud y longitud\n",
        "gdf['latitud'] = gdf.geometry.y\n",
        "gdf['longitud'] = gdf.geometry.x\n"
      ],
      "metadata": {
        "id": "--7l1BTeGEtQ"
      },
      "id": "--7l1BTeGEtQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Convertir cluster a string para usar color_discrete_map\n",
        "gdf['cluster_str'] = gdf['cluster'].astype(str)\n",
        "\n",
        "color_discrete_map = {\n",
        "    \"0\": \"red\",\n",
        "    \"1\": \"green\"\n",
        "}\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf,\n",
        "    lat=\"latitud\",\n",
        "    lon=\"longitud\",\n",
        "    color=\"cluster_str\",  # Usar la columna de strings\n",
        "    zoom=11,\n",
        "    height=600,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    hover_data={\n",
        "        \"media_ocupacion\": True,\n",
        "        \"numero_plazas\": True\n",
        "    },\n",
        "    color_discrete_map=color_discrete_map\n",
        ")\n",
        "\n",
        "# Reducir tamaño de los puntos\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "\n",
        "fig.update_layout(title=\"Clusters de Zonas de Aparcamiento\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "g-cgopRT2uTU"
      },
      "id": "g-cgopRT2uTU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo\n",
        "joblib.dump(kmeans, 'kmeans_mañana_entre_semana.pkl')"
      ],
      "metadata": {
        "id": "tA--mmhhnmf4"
      },
      "id": "tA--mmhhnmf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suponiendo que bank_clust es tu dataframe de entrada\n",
        "gdf_tree = gdf.copy()\n",
        "\n",
        "# Asegurar que 'cluster' sea numérico\n",
        "gdf_tree[\"cluster\"] = gdf_tree[\"cluster\"].astype(int)\n",
        "\n",
        "# Crear variables dummy estilo one-vs-rest\n",
        "gdf_tree[\"cluster_1\"] = (gdf_tree[\"cluster\"] == 1).astype(int)\n",
        "gdf_tree[\"cluster_2\"] = (gdf_tree[\"cluster\"] == 2).astype(int)\n",
        "gdf_tree[\"cluster_3\"] = (gdf_tree[\"cluster\"] == 3).astype(int)"
      ],
      "metadata": {
        "id": "ZUNnlES4r0Ds"
      },
      "id": "ZUNnlES4r0Ds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_tree"
      ],
      "metadata": {
        "id": "2Fook67LsZKd"
      },
      "id": "2Fook67LsZKd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionamos las variables predictoras y la variable objetivo\n",
        "X = gdf_tree[['media_minutos', 'num_registros', 'numero_plazas','color','bateria_linea','log_ratio_intensidad_por_nplazas','media_ocupacion']] # columnas 0 a 16 (como en R: 1:17)\n",
        "y = gdf_tree[\"cluster_1\"]\n",
        "\n",
        "# Entrenamos el árbol\n",
        "tree_clf = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "tree_clf.fit(X, y)\n"
      ],
      "metadata": {
        "id": "hMhrtEHFsXrP"
      },
      "id": "hMhrtEHFsXrP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "# Ajustamos el tamaño de la figura\n",
        "plt.figure(figsize=(24, 12))\n",
        "\n",
        "# Dibujamos el árbol con etiquetas y colores\n",
        "plot_tree(\n",
        "    tree_clf,\n",
        "    feature_names=X.columns,\n",
        "    class_names=[\"No Cluster 1\", \"Cluster 1\"],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=12,\n",
        "    proportion=True,  # puedes poner True para que las cajas tengan tamaños proporcionales\n",
        ")\n",
        "\n",
        "plt.title(\"VARIABLES ASOCIADAS A CLÚSTER - 1\", fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ma3wh3CQsq9D"
      },
      "id": "Ma3wh3CQsq9D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering para un día entre semana al mediodia"
      ],
      "metadata": {
        "id": "cOZAIV2CnRoG"
      },
      "id": "cOZAIV2CnRoG"
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_1 = df_parking_scaled[(df_parking_scaled['int_tiempo'] == 1) & (df_parking_scaled['fin_de_semana'] == 0)]\n",
        "df_parking_1_ok = df_parking_1[['media_minutos', 'num_registros', 'numero_plazas','color','bateria_linea','log_ratio_intensidad_por_nplazas','media_ocupacion']]"
      ],
      "metadata": {
        "id": "zfwBDcMknXo2"
      },
      "id": "zfwBDcMknXo2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PCA object with 6 components\n",
        "pca = PCA(n_components=4)\n",
        "\n",
        "# Fitting and transforming the original data to the new PCA dataframe\n",
        "df_parking_pca = pca.fit_transform(df_parking_1_ok)\n",
        "\n",
        "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
        "df_parking_pca = pd.DataFrame(df_parking_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
        "\n",
        "# Adding the CustomerID index back to the new PCA dataframe\n",
        "df_parking_pca.index = df_parking_1.index"
      ],
      "metadata": {
        "id": "PGa7ptJ5p2d_"
      },
      "id": "PGa7ptJ5p2d_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 1. ¿Qué es la inercia en KMeans?\n",
        "La inercia mide cuán cerca están los puntos de su centroide asignado. Es la suma de las distancias cuadradas de cada punto al centroide del cluster al que pertenece.\n",
        "\n",
        "Menor inercia = clusters más compactos (pero no necesariamente mejores).\n",
        "\n",
        "Siempre disminuye al aumentar k, por eso no se puede usar sola para elegir k.\n",
        "\n",
        "📌 2. ¿Qué es el Silhouette Score?\n",
        "El Silhouette Score mide qué tan bien separado está un punto de los clusters vecinos. Su valor va de -1 a 1.\n",
        "\n",
        "Cerca de 1: los puntos están bien agrupados y separados de otros clusters.\n",
        "\n",
        "Cerca de 0: los puntos están cerca del límite entre clusters.\n",
        "\n",
        "Cerca de -1: los puntos están probablemente mal asignados.\n",
        "\n",
        "🤔 ¿Qué significa tener un valor de k con alta inercia pero buen Silhouette Score?\n",
        "Eso indica que:\n",
        "\n",
        "Los clusters están bien separados y definidos (silhouette score alto),\n",
        "\n",
        "Pero no necesariamente muy compactos (porque la inercia aún es alta).\n",
        "\n",
        "Esto puede pasar cuando:\n",
        "\n",
        "Hay pocos clusters, y aún así los puntos están bien agrupados (aunque los clusters sean grandes, poco densos).\n",
        "\n",
        "O los datos están naturalmente bien separados, incluso con k pequeño.\n",
        "\n",
        "✅ ¿Qué deberías hacer en ese caso?\n",
        "Confía más en el Silhouette Score que en la inercia.\n",
        "\n",
        "La inercia te ayuda a ver la tendencia, pero el Silhouette Score te dice qué tan buena es la partición.\n",
        "\n",
        "Un k con buena separación (alto silhouette) es preferible aunque la inercia no sea mínima.\n",
        "\n"
      ],
      "metadata": {
        "id": "TMN8LyYZq_5k"
      },
      "id": "TMN8LyYZq_5k"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Solo columnas numéricas\n",
        "X = df_parking_pca.select_dtypes(include=[np.number])\n",
        "\n",
        "# Rango de k a probar\n",
        "k_values = range(2, 8)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouettes.append(silhouette_score(X, labels))\n",
        "\n",
        "# Elegir mejor k según silhouette\n",
        "best_k_silhouette = k_values[np.argmax(silhouettes)]\n",
        "\n",
        "# --------- Gráfico 1: Elbow (Inercia) ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, inertias, marker='o', color='darkorange')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Método del codo (Elbow)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Gráfico 2: Silhouette ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, silhouettes, marker='s', linestyle='--', color='teal')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score por número de clusters')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Resultado óptimo ----------\n",
        "print(f'✅ Mejor k según silhouette score: {best_k_silhouette} (score = {max(silhouettes):.3f})')\n"
      ],
      "metadata": {
        "id": "97aH4GWBp9jE"
      },
      "id": "97aH4GWBp9jE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "\n",
        "# Apply KMeans clustering using the optimal k\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "kmeans.fit(df_parking_pca)\n",
        "\n",
        "# Get the frequency of each cluster\n",
        "cluster_frequencies = Counter(kmeans.labels_)\n",
        "\n",
        "# Append the new cluster labels back to the original dataset\n",
        "df_parking_1['cluster'] = kmeans.labels_\n",
        "# Append the new cluster labels to the PCA version of the dataset\n",
        "df_parking_pca['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "Nmt5chFVqHY7"
      },
      "id": "Nmt5chFVqHY7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each cluster\n",
        "cluster_percentage = (df_parking_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h')\n",
        "\n",
        "# Adding percentages on the bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "am9yKP2hqNPD"
      },
      "id": "am9yKP2hqNPD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats_por_cluster = df_parking_1.drop('geometria', axis=1).groupby('cluster').mean()\n",
        "\n",
        "stats_por_cluster"
      ],
      "metadata": {
        "id": "vlIbRt_Q7QV9"
      },
      "id": "vlIbRt_Q7QV9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "# 1. Convertir columna de geometría (WKT -> Point)\n",
        "df_parking_1['geometria'] = df_parking_1['geometria'].apply(wkt.loads)\n",
        "\n",
        "# 2. Crear GeoDataFrame con CRS EPSG:25830\n",
        "gdf = gpd.GeoDataFrame(df_parking_1, geometry='geometria', crs='EPSG:25830')\n",
        "\n",
        "# 3. Reproyectar a EPSG:4326 (lat/lon)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# 4. Extraer latitud y longitud\n",
        "gdf['latitud'] = gdf.geometry.y\n",
        "gdf['longitud'] = gdf.geometry.x\n",
        "\n",
        "# Convertir cluster a string para usar color_discrete_map\n",
        "gdf['cluster_str'] = gdf['cluster'].astype(str)\n",
        "\n",
        "color_discrete_map = {\n",
        "    \"0\": \"red\",\n",
        "    \"1\": \"orange\",\n",
        "    \"2\" : \"green\"\n",
        "}\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf,\n",
        "    lat=\"latitud\",\n",
        "    lon=\"longitud\",\n",
        "    color=\"cluster_str\",  # Usar la columna de strings\n",
        "    zoom=11,\n",
        "    height=600,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    hover_data={\n",
        "        \"media_ocupacion\": True,\n",
        "        \"numero_plazas\": True\n",
        "    },\n",
        "    color_discrete_map=color_discrete_map\n",
        ")\n",
        "\n",
        "# Reducir tamaño de los puntos\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "\n",
        "fig.update_layout(title=\"Clusters de Zonas de Aparcamiento\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "TyrJjhnQrD96"
      },
      "id": "TyrJjhnQrD96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos observar que debido al tráfico que hay en Madrid Capital un día entresemana al mediodía las plazas verdes se reducen bastante mientras que aumentan las naranjas y las rojas."
      ],
      "metadata": {
        "id": "FRHBNkNDrbBR"
      },
      "id": "FRHBNkNDrbBR"
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo\n",
        "joblib.dump(kmeans, 'kmeans_mañana_entre_semana.pkl')"
      ],
      "metadata": {
        "id": "ra0cbaIgsGx_"
      },
      "id": "ra0cbaIgsGx_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering de un día entre semana por la tarde"
      ],
      "metadata": {
        "id": "D6cmb9xisHKN"
      },
      "id": "D6cmb9xisHKN"
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_1 = df_parking_scaled[(df_parking_scaled['int_tiempo'] == 2) & (df_parking_scaled['fin_de_semana'] == 0)]\n",
        "df_parking_1_ok = df_parking_1[['media_minutos', 'num_registros', 'numero_plazas','color','bateria_linea','log_ratio_intensidad_por_nplazas','media_ocupacion']]"
      ],
      "metadata": {
        "id": "e8A5tnKwsSiK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "e8A5tnKwsSiK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PCA object with 6 components\n",
        "pca = PCA(n_components=4)\n",
        "\n",
        "# Fitting and transforming the original data to the new PCA dataframe\n",
        "df_parking_pca = pca.fit_transform(df_parking_1_ok)\n",
        "\n",
        "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
        "df_parking_pca = pd.DataFrame(df_parking_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
        "\n",
        "# Adding the CustomerID index back to the new PCA dataframe\n",
        "df_parking_pca.index = df_parking_1.index"
      ],
      "metadata": {
        "id": "IGUlrjO2sSiL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IGUlrjO2sSiL"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Solo columnas numéricas\n",
        "X = df_parking_pca.select_dtypes(include=[np.number])\n",
        "\n",
        "# Rango de k a probar\n",
        "k_values = range(2, 8)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouettes.append(silhouette_score(X, labels))\n",
        "\n",
        "# Elegir mejor k según silhouette\n",
        "best_k_silhouette = k_values[np.argmax(silhouettes)]\n",
        "\n",
        "# --------- Gráfico 1: Elbow (Inercia) ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, inertias, marker='o', color='darkorange')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Método del codo (Elbow)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Gráfico 2: Silhouette ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, silhouettes, marker='s', linestyle='--', color='teal')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score por número de clusters')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Resultado óptimo ----------\n",
        "print(f'✅ Mejor k según silhouette score: {best_k_silhouette} (score = {max(silhouettes):.3f})')\n"
      ],
      "metadata": {
        "id": "Pa7itFStsSiL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Pa7itFStsSiL"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "\n",
        "# Apply KMeans clustering using the optimal k\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "kmeans.fit(df_parking_pca)\n",
        "\n",
        "# Get the frequency of each cluster\n",
        "cluster_frequencies = Counter(kmeans.labels_)\n",
        "\n",
        "# Append the new cluster labels back to the original dataset\n",
        "df_parking_1['cluster'] = kmeans.labels_\n",
        "# Append the new cluster labels to the PCA version of the dataset\n",
        "df_parking_pca['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "XREirKFBsSiL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XREirKFBsSiL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each cluster\n",
        "cluster_percentage = (df_parking_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h')\n",
        "\n",
        "# Adding percentages on the bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3xnB7XD7sSiL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3xnB7XD7sSiL"
    },
    {
      "cell_type": "code",
      "source": [
        "stats_por_cluster = df_parking_1.drop('geometria', axis=1).groupby('cluster').mean()\n",
        "\n",
        "stats_por_cluster"
      ],
      "metadata": {
        "id": "mQj0GvvnsSiL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mQj0GvvnsSiL"
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "# 1. Convertir columna de geometría (WKT -> Point)\n",
        "#df_parking_1['geometria'] = df_parking_1['geometria'].apply(wkt.loads)\n",
        "\n",
        "# 2. Crear GeoDataFrame con CRS EPSG:25830\n",
        "gdf = gpd.GeoDataFrame(df_parking_1, geometry='geometria', crs='EPSG:25830')\n",
        "\n",
        "# 3. Reproyectar a EPSG:4326 (lat/lon)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# 4. Extraer latitud y longitud\n",
        "gdf['latitud'] = gdf.geometry.y\n",
        "gdf['longitud'] = gdf.geometry.x\n",
        "\n",
        "# Convertir cluster a string para usar color_discrete_map\n",
        "gdf['cluster_str'] = gdf['cluster'].astype(str)\n",
        "\n",
        "color_discrete_map = {\n",
        "    \"0\": \"green\",\n",
        "    \"1\": \"orange\",\n",
        "    \"2\" : \"red\"\n",
        "}\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf,\n",
        "    lat=\"latitud\",\n",
        "    lon=\"longitud\",\n",
        "    color=\"cluster_str\",  # Usar la columna de strings\n",
        "    zoom=11,\n",
        "    height=600,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    hover_data={\n",
        "        \"media_ocupacion\": True,\n",
        "        \"numero_plazas\": True\n",
        "    },\n",
        "    color_discrete_map=color_discrete_map\n",
        ")\n",
        "\n",
        "# Reducir tamaño de los puntos\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "\n",
        "fig.update_layout(title=\"Clusters de Zonas de Aparcamiento\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "4rU50fcqsSiM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4rU50fcqsSiM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que muchas de esas zonas verdes se recuperan por la tarde al disminuir el tráfico."
      ],
      "metadata": {
        "id": "S_P31kVTtLMk"
      },
      "id": "S_P31kVTtLMk"
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo\n",
        "joblib.dump(kmeans, 'kmeans_tarde_entre_semana.pkl')"
      ],
      "metadata": {
        "id": "X-qoUi4LsSiM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "X-qoUi4LsSiM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering para un día entre semana por la noche"
      ],
      "metadata": {
        "id": "4b1ejZkVxwBr"
      },
      "id": "4b1ejZkVxwBr"
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_1 = df_parking_scaled[(df_parking_scaled['int_tiempo'] == 3) & (df_parking_scaled['fin_de_semana'] == 0)]\n",
        "df_parking_1_ok = df_parking_1[['media_minutos', 'num_registros', 'numero_plazas','color','bateria_linea','log_ratio_intensidad_por_nplazas','media_ocupacion']]"
      ],
      "metadata": {
        "id": "G2aXd-pWx6dD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "G2aXd-pWx6dD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PCA object with 6 components\n",
        "pca = PCA(n_components=4)\n",
        "\n",
        "# Fitting and transforming the original data to the new PCA dataframe\n",
        "df_parking_pca = pca.fit_transform(df_parking_1_ok)\n",
        "\n",
        "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
        "df_parking_pca = pd.DataFrame(df_parking_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
        "\n",
        "# Adding the CustomerID index back to the new PCA dataframe\n",
        "df_parking_pca.index = df_parking_1.index"
      ],
      "metadata": {
        "id": "wfjUnKe8x6dE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wfjUnKe8x6dE"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Solo columnas numéricas\n",
        "X = df_parking_pca.select_dtypes(include=[np.number])\n",
        "\n",
        "# Rango de k a probar\n",
        "k_values = range(2, 8)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouettes.append(silhouette_score(X, labels))\n",
        "\n",
        "# Elegir mejor k según silhouette\n",
        "best_k_silhouette = k_values[np.argmax(silhouettes)]\n",
        "\n",
        "# --------- Gráfico 1: Elbow (Inercia) ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, inertias, marker='o', color='darkorange')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Método del codo (Elbow)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Gráfico 2: Silhouette ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, silhouettes, marker='s', linestyle='--', color='teal')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score por número de clusters')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Resultado óptimo ----------\n",
        "print(f'✅ Mejor k según silhouette score: {best_k_silhouette} (score = {max(silhouettes):.3f})')\n"
      ],
      "metadata": {
        "id": "1WSo0lWCx6dE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1WSo0lWCx6dE"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "\n",
        "# Apply KMeans clustering using the optimal k\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "kmeans.fit(df_parking_pca)\n",
        "\n",
        "# Get the frequency of each cluster\n",
        "cluster_frequencies = Counter(kmeans.labels_)\n",
        "\n",
        "# Append the new cluster labels back to the original dataset\n",
        "df_parking_1['cluster'] = kmeans.labels_\n",
        "# Append the new cluster labels to the PCA version of the dataset\n",
        "df_parking_pca['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "DX3lF-JFx6dE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DX3lF-JFx6dE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each cluster\n",
        "cluster_percentage = (df_parking_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h')\n",
        "\n",
        "# Adding percentages on the bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ahHiVx2zx6dE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ahHiVx2zx6dE"
    },
    {
      "cell_type": "code",
      "source": [
        "stats_por_cluster = df_parking_1.drop('geometria', axis=1).groupby('cluster').mean()\n",
        "\n",
        "stats_por_cluster"
      ],
      "metadata": {
        "id": "D-v2VG4tx6dF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "D-v2VG4tx6dF"
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "# 1. Convertir columna de geometría (WKT -> Point)\n",
        "#df_parking_1['geometria'] = df_parking_1['geometria'].apply(wkt.loads)\n",
        "\n",
        "# 2. Crear GeoDataFrame con CRS EPSG:25830\n",
        "gdf = gpd.GeoDataFrame(df_parking_1, geometry='geometria', crs='EPSG:25830')\n",
        "\n",
        "# 3. Reproyectar a EPSG:4326 (lat/lon)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# 4. Extraer latitud y longitud\n",
        "gdf['latitud'] = gdf.geometry.y\n",
        "gdf['longitud'] = gdf.geometry.x\n",
        "\n",
        "# Convertir cluster a string para usar color_discrete_map\n",
        "gdf['cluster_str'] = gdf['cluster'].astype(str)\n",
        "\n",
        "color_discrete_map = {\n",
        "    \"0\": \"green\",\n",
        "    \"1\": \"orange\",\n",
        "    \"2\" : \"red\"\n",
        "}\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf,\n",
        "    lat=\"latitud\",\n",
        "    lon=\"longitud\",\n",
        "    color=\"cluster_str\",  # Usar la columna de strings\n",
        "    zoom=11,\n",
        "    height=600,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    hover_data={\n",
        "        \"media_ocupacion\": True,\n",
        "        \"numero_plazas\": True\n",
        "    },\n",
        "    color_discrete_map=color_discrete_map\n",
        ")\n",
        "\n",
        "# Reducir tamaño de los puntos\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "\n",
        "fig.update_layout(title=\"Clusters de Zonas de Aparcamiento\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "eOFFPuorx6dF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eOFFPuorx6dF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que muchas de esas zonas verdes se recuperan por la tarde al disminuir el tráfico."
      ],
      "metadata": {
        "id": "IHgn2WpXx6dF"
      },
      "id": "IHgn2WpXx6dF"
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo\n",
        "joblib.dump(kmeans, 'kmeans_tarde_entre_semana.pkl')"
      ],
      "metadata": {
        "id": "doteEQWix6dF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "doteEQWix6dF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fin de semana por la mañana"
      ],
      "metadata": {
        "id": "gqKsc8NLx6dF"
      },
      "id": "gqKsc8NLx6dF"
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_1 = df_parking_scaled[(df_parking_scaled['int_tiempo'] == 0) & (df_parking_scaled['fin_de_semana'] == 1)]\n",
        "df_parking_1_ok = df_parking_1[['media_minutos', 'num_registros', 'numero_plazas','color','bateria_linea','log_ratio_intensidad_por_nplazas','media_ocupacion']]"
      ],
      "metadata": {
        "id": "7jWi-1tjywec"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7jWi-1tjywec"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PCA object with 6 components\n",
        "pca = PCA(n_components=4)\n",
        "\n",
        "# Fitting and transforming the original data to the new PCA dataframe\n",
        "df_parking_pca = pca.fit_transform(df_parking_1_ok)\n",
        "\n",
        "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
        "df_parking_pca = pd.DataFrame(df_parking_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
        "\n",
        "# Adding the CustomerID index back to the new PCA dataframe\n",
        "df_parking_pca.index = df_parking_1.index"
      ],
      "metadata": {
        "id": "0B9jXR3Qywed"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0B9jXR3Qywed"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Solo columnas numéricas\n",
        "X = df_parking_pca.select_dtypes(include=[np.number])\n",
        "\n",
        "# Rango de k a probar\n",
        "k_values = range(2, 8)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouettes.append(silhouette_score(X, labels))\n",
        "\n",
        "# Elegir mejor k según silhouette\n",
        "best_k_silhouette = k_values[np.argmax(silhouettes)]\n",
        "\n",
        "# --------- Gráfico 1: Elbow (Inercia) ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, inertias, marker='o', color='darkorange')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Método del codo (Elbow)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Gráfico 2: Silhouette ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, silhouettes, marker='s', linestyle='--', color='teal')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score por número de clusters')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Resultado óptimo ----------\n",
        "print(f'✅ Mejor k según silhouette score: {best_k_silhouette} (score = {max(silhouettes):.3f})')\n"
      ],
      "metadata": {
        "id": "j7SHBwVLywed"
      },
      "execution_count": null,
      "outputs": [],
      "id": "j7SHBwVLywed"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "\n",
        "# Apply KMeans clustering using the optimal k\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "kmeans.fit(df_parking_pca)\n",
        "\n",
        "# Get the frequency of each cluster\n",
        "cluster_frequencies = Counter(kmeans.labels_)\n",
        "\n",
        "# Append the new cluster labels back to the original dataset\n",
        "df_parking_1['cluster'] = kmeans.labels_\n",
        "# Append the new cluster labels to the PCA version of the dataset\n",
        "df_parking_pca['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "-92xycplywed"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-92xycplywed"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each cluster\n",
        "cluster_percentage = (df_parking_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h')\n",
        "\n",
        "# Adding percentages on the bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D7SmWZZGywed"
      },
      "execution_count": null,
      "outputs": [],
      "id": "D7SmWZZGywed"
    },
    {
      "cell_type": "code",
      "source": [
        "stats_por_cluster = df_parking_1.drop('geometria', axis=1).groupby('cluster').mean()\n",
        "\n",
        "stats_por_cluster"
      ],
      "metadata": {
        "id": "1KIkGTupywed"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1KIkGTupywed"
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "# 1. Convertir columna de geometría (WKT -> Point)\n",
        "#df_parking_1['geometria'] = df_parking_1['geometria'].apply(wkt.loads)\n",
        "\n",
        "# 2. Crear GeoDataFrame con CRS EPSG:25830\n",
        "gdf = gpd.GeoDataFrame(df_parking_1, geometry='geometria', crs='EPSG:25830')\n",
        "\n",
        "# 3. Reproyectar a EPSG:4326 (lat/lon)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# 4. Extraer latitud y longitud\n",
        "gdf['latitud'] = gdf.geometry.y\n",
        "gdf['longitud'] = gdf.geometry.x\n",
        "\n",
        "# Convertir cluster a string para usar color_discrete_map\n",
        "gdf['cluster_str'] = gdf['cluster'].astype(str)\n",
        "\n",
        "color_discrete_map = {\n",
        "    \"0\": \"green\",\n",
        "    \"1\": \"orange\",\n",
        "    \"2\" : \"red\"\n",
        "}\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf,\n",
        "    lat=\"latitud\",\n",
        "    lon=\"longitud\",\n",
        "    color=\"cluster_str\",  # Usar la columna de strings\n",
        "    zoom=11,\n",
        "    height=600,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    hover_data={\n",
        "        \"media_ocupacion\": True,\n",
        "        \"numero_plazas\": True\n",
        "    },\n",
        "    color_discrete_map=color_discrete_map\n",
        ")\n",
        "\n",
        "# Reducir tamaño de los puntos\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "\n",
        "fig.update_layout(title=\"Clusters de Zonas de Aparcamiento\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "w_R7Ygyuywee"
      },
      "execution_count": null,
      "outputs": [],
      "id": "w_R7Ygyuywee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que muchas de esas zonas verdes se recuperan por la tarde al disminuir el tráfico."
      ],
      "metadata": {
        "id": "AQyIsHBpywee"
      },
      "id": "AQyIsHBpywee"
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo\n",
        "joblib.dump(kmeans, 'kmeans_tarde_entre_semana.pkl')"
      ],
      "metadata": {
        "id": "A47vPPayywee"
      },
      "execution_count": null,
      "outputs": [],
      "id": "A47vPPayywee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estudio clustering fin de semana medio día"
      ],
      "metadata": {
        "id": "Oe_9iSLn1c6Y"
      },
      "id": "Oe_9iSLn1c6Y"
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_1 = df_parking_scaled[(df_parking_scaled['int_tiempo'] == 1) & (df_parking_scaled['fin_de_semana'] == 1)]\n",
        "df_parking_1_ok = df_parking_1[['media_minutos', 'num_registros', 'numero_plazas','color','bateria_linea','log_ratio_intensidad_por_nplazas','media_ocupacion']]"
      ],
      "metadata": {
        "id": "Wv2v6mVP1bDI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Wv2v6mVP1bDI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PCA object with 6 components\n",
        "pca = PCA(n_components=4)\n",
        "\n",
        "# Fitting and transforming the original data to the new PCA dataframe\n",
        "df_parking_pca = pca.fit_transform(df_parking_1_ok)\n",
        "\n",
        "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
        "df_parking_pca = pd.DataFrame(df_parking_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
        "\n",
        "# Adding the CustomerID index back to the new PCA dataframe\n",
        "df_parking_pca.index = df_parking_1.index"
      ],
      "metadata": {
        "id": "eXljPNKR1bDI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eXljPNKR1bDI"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Solo columnas numéricas\n",
        "X = df_parking_pca.select_dtypes(include=[np.number])\n",
        "\n",
        "# Rango de k a probar\n",
        "k_values = range(2, 8)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouettes.append(silhouette_score(X, labels))\n",
        "\n",
        "# Elegir mejor k según silhouette\n",
        "best_k_silhouette = k_values[np.argmax(silhouettes)]\n",
        "\n",
        "# --------- Gráfico 1: Elbow (Inercia) ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, inertias, marker='o', color='darkorange')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Método del codo (Elbow)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Gráfico 2: Silhouette ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, silhouettes, marker='s', linestyle='--', color='teal')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score por número de clusters')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Resultado óptimo ----------\n",
        "print(f'✅ Mejor k según silhouette score: {best_k_silhouette} (score = {max(silhouettes):.3f})')\n"
      ],
      "metadata": {
        "id": "R-85IbK71bDJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "R-85IbK71bDJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "\n",
        "# Apply KMeans clustering using the optimal k\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "kmeans.fit(df_parking_pca)\n",
        "\n",
        "# Get the frequency of each cluster\n",
        "cluster_frequencies = Counter(kmeans.labels_)\n",
        "\n",
        "# Append the new cluster labels back to the original dataset\n",
        "df_parking_1['cluster'] = kmeans.labels_\n",
        "# Append the new cluster labels to the PCA version of the dataset\n",
        "df_parking_pca['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "Y0PK4EsX1bDJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Y0PK4EsX1bDJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each cluster\n",
        "cluster_percentage = (df_parking_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h')\n",
        "\n",
        "# Adding percentages on the bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aWOTXuoB1bDJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aWOTXuoB1bDJ"
    },
    {
      "cell_type": "code",
      "source": [
        "stats_por_cluster = df_parking_1.drop('geometria', axis=1).groupby('cluster').mean()\n",
        "\n",
        "stats_por_cluster"
      ],
      "metadata": {
        "id": "20ndvgL31bDJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "20ndvgL31bDJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "# 1. Convertir columna de geometría (WKT -> Point)\n",
        "#df_parking_1['geometria'] = df_parking_1['geometria'].apply(wkt.loads)\n",
        "\n",
        "# 2. Crear GeoDataFrame con CRS EPSG:25830\n",
        "gdf = gpd.GeoDataFrame(df_parking_1, geometry='geometria', crs='EPSG:25830')\n",
        "\n",
        "# 3. Reproyectar a EPSG:4326 (lat/lon)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# 4. Extraer latitud y longitud\n",
        "gdf['latitud'] = gdf.geometry.y\n",
        "gdf['longitud'] = gdf.geometry.x\n",
        "\n",
        "# Convertir cluster a string para usar color_discrete_map\n",
        "gdf['cluster_str'] = gdf['cluster'].astype(str)\n",
        "\n",
        "color_discrete_map = {\n",
        "    \"0\": \"green\",\n",
        "    \"1\": \"orange\",\n",
        "    \"2\" : \"red\"\n",
        "}\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf,\n",
        "    lat=\"latitud\",\n",
        "    lon=\"longitud\",\n",
        "    color=\"cluster_str\",  # Usar la columna de strings\n",
        "    zoom=11,\n",
        "    height=600,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    hover_data={\n",
        "        \"media_ocupacion\": True,\n",
        "        \"numero_plazas\": True\n",
        "    },\n",
        "    color_discrete_map=color_discrete_map\n",
        ")\n",
        "\n",
        "# Reducir tamaño de los puntos\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "\n",
        "fig.update_layout(title=\"Clusters de Zonas de Aparcamiento\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "T-TRUPCA1bDK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "T-TRUPCA1bDK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que muchas de esas zonas verdes se recuperan por la tarde al disminuir el tráfico."
      ],
      "metadata": {
        "id": "emzU51XD1bDK"
      },
      "id": "emzU51XD1bDK"
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo\n",
        "joblib.dump(kmeans, 'kmeans_tarde_entre_semana.pkl')"
      ],
      "metadata": {
        "id": "XuA5Dth21bDK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XuA5Dth21bDK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering de zonas SER un fin de semana por la tarde"
      ],
      "metadata": {
        "id": "aNr9NCh62TM_"
      },
      "id": "aNr9NCh62TM_"
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_1 = df_parking_scaled[(df_parking_scaled['int_tiempo'] == 2) & (df_parking_scaled['fin_de_semana'] == 1)]\n",
        "df_parking_1_ok = df_parking_1[['media_minutos', 'num_registros', 'numero_plazas','color','bateria_linea','log_ratio_intensidad_por_nplazas','media_ocupacion']]"
      ],
      "metadata": {
        "id": "GQogPDBa2XVF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GQogPDBa2XVF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PCA object with 6 components\n",
        "pca = PCA(n_components=4)\n",
        "\n",
        "# Fitting and transforming the original data to the new PCA dataframe\n",
        "df_parking_pca = pca.fit_transform(df_parking_1_ok)\n",
        "\n",
        "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
        "df_parking_pca = pd.DataFrame(df_parking_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
        "\n",
        "# Adding the CustomerID index back to the new PCA dataframe\n",
        "df_parking_pca.index = df_parking_1.index"
      ],
      "metadata": {
        "id": "hmIcZ_f82XVF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hmIcZ_f82XVF"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Solo columnas numéricas\n",
        "X = df_parking_pca.select_dtypes(include=[np.number])\n",
        "\n",
        "# Rango de k a probar\n",
        "k_values = range(2, 8)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouettes.append(silhouette_score(X, labels))\n",
        "\n",
        "# Elegir mejor k según silhouette\n",
        "best_k_silhouette = k_values[np.argmax(silhouettes)]\n",
        "\n",
        "# --------- Gráfico 1: Elbow (Inercia) ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, inertias, marker='o', color='darkorange')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Método del codo (Elbow)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Gráfico 2: Silhouette ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, silhouettes, marker='s', linestyle='--', color='teal')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score por número de clusters')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Resultado óptimo ----------\n",
        "print(f'✅ Mejor k según silhouette score: {best_k_silhouette} (score = {max(silhouettes):.3f})')\n"
      ],
      "metadata": {
        "id": "qU4u1g_k2XVG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qU4u1g_k2XVG"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "\n",
        "# Apply KMeans clustering using the optimal k\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "kmeans.fit(df_parking_pca)\n",
        "\n",
        "# Get the frequency of each cluster\n",
        "cluster_frequencies = Counter(kmeans.labels_)\n",
        "\n",
        "# Append the new cluster labels back to the original dataset\n",
        "df_parking_1['cluster'] = kmeans.labels_\n",
        "# Append the new cluster labels to the PCA version of the dataset\n",
        "df_parking_pca['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "5U0GGT9g2XVG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5U0GGT9g2XVG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each cluster\n",
        "cluster_percentage = (df_parking_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h')\n",
        "\n",
        "# Adding percentages on the bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ez5G1oci2XVG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ez5G1oci2XVG"
    },
    {
      "cell_type": "code",
      "source": [
        "stats_por_cluster = df_parking_1.drop('geometria', axis=1).groupby('cluster').mean()\n",
        "\n",
        "stats_por_cluster"
      ],
      "metadata": {
        "id": "EYHq_MNj2XVG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EYHq_MNj2XVG"
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "# 1. Convertir columna de geometría (WKT -> Point)\n",
        "#df_parking_1['geometria'] = df_parking_1['geometria'].apply(wkt.loads)\n",
        "\n",
        "# 2. Crear GeoDataFrame con CRS EPSG:25830\n",
        "gdf = gpd.GeoDataFrame(df_parking_1, geometry='geometria', crs='EPSG:25830')\n",
        "\n",
        "# 3. Reproyectar a EPSG:4326 (lat/lon)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# 4. Extraer latitud y longitud\n",
        "gdf['latitud'] = gdf.geometry.y\n",
        "gdf['longitud'] = gdf.geometry.x\n",
        "\n",
        "# Convertir cluster a string para usar color_discrete_map\n",
        "gdf['cluster_str'] = gdf['cluster'].astype(str)\n",
        "\n",
        "color_discrete_map = {\n",
        "    \"0\": \"green\",\n",
        "    \"1\": \"orange\",\n",
        "    \"2\" : \"red\"\n",
        "}\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf,\n",
        "    lat=\"latitud\",\n",
        "    lon=\"longitud\",\n",
        "    color=\"cluster_str\",  # Usar la columna de strings\n",
        "    zoom=11,\n",
        "    height=600,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    hover_data={\n",
        "        \"media_ocupacion\": True,\n",
        "        \"numero_plazas\": True\n",
        "    },\n",
        "    color_discrete_map=color_discrete_map\n",
        ")\n",
        "\n",
        "# Reducir tamaño de los puntos\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "\n",
        "fig.update_layout(title=\"Clusters de Zonas de Aparcamiento\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "SfpZYmoD2XVG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "SfpZYmoD2XVG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que muchas de esas zonas verdes se recuperan por la tarde al disminuir el tráfico."
      ],
      "metadata": {
        "id": "pfDH56Qj2XVG"
      },
      "id": "pfDH56Qj2XVG"
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo\n",
        "joblib.dump(kmeans, 'kmeans_tarde_entre_semana.pkl')"
      ],
      "metadata": {
        "id": "t3wSXE3s2XVH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "t3wSXE3s2XVH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering de zonas SER de un fin de semana por la noche"
      ],
      "metadata": {
        "id": "7kbympl62blp"
      },
      "id": "7kbympl62blp"
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_1 = df_parking_scaled[(df_parking_scaled['int_tiempo'] == 3) & (df_parking_scaled['fin_de_semana'] == 1)]\n",
        "df_parking_1_ok = df_parking_1[['media_minutos', 'num_registros', 'numero_plazas','color','bateria_linea','log_ratio_intensidad_por_nplazas','media_ocupacion']]"
      ],
      "metadata": {
        "id": "kNBpZaKU2hrc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kNBpZaKU2hrc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PCA object with 6 components\n",
        "pca = PCA(n_components=4)\n",
        "\n",
        "# Fitting and transforming the original data to the new PCA dataframe\n",
        "df_parking_pca = pca.fit_transform(df_parking_1_ok)\n",
        "\n",
        "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
        "df_parking_pca = pd.DataFrame(df_parking_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
        "\n",
        "# Adding the CustomerID index back to the new PCA dataframe\n",
        "df_parking_pca.index = df_parking_1.index"
      ],
      "metadata": {
        "id": "K1LoOUVP2hrd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "K1LoOUVP2hrd"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Solo columnas numéricas\n",
        "X = df_parking_pca.select_dtypes(include=[np.number])\n",
        "\n",
        "# Rango de k a probar\n",
        "k_values = range(2, 8)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouettes.append(silhouette_score(X, labels))\n",
        "\n",
        "# Elegir mejor k según silhouette\n",
        "best_k_silhouette = k_values[np.argmax(silhouettes)]\n",
        "\n",
        "# --------- Gráfico 1: Elbow (Inercia) ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, inertias, marker='o', color='darkorange')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Método del codo (Elbow)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Gráfico 2: Silhouette ----------\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, silhouettes, marker='s', linestyle='--', color='teal')\n",
        "plt.xlabel('Número de clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score por número de clusters')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Resultado óptimo ----------\n",
        "print(f'✅ Mejor k según silhouette score: {best_k_silhouette} (score = {max(silhouettes):.3f})')\n"
      ],
      "metadata": {
        "id": "48DzkOuA2hrd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "48DzkOuA2hrd"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "\n",
        "# Apply KMeans clustering using the optimal k\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "kmeans.fit(df_parking_pca)\n",
        "\n",
        "# Get the frequency of each cluster\n",
        "cluster_frequencies = Counter(kmeans.labels_)\n",
        "\n",
        "# Append the new cluster labels back to the original dataset\n",
        "df_parking_1['cluster'] = kmeans.labels_\n",
        "# Append the new cluster labels to the PCA version of the dataset\n",
        "df_parking_pca['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "94orRh6S2hrd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "94orRh6S2hrd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each cluster\n",
        "cluster_percentage = (df_parking_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h')\n",
        "\n",
        "# Adding percentages on the bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "67sNDW_C2hrd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "67sNDW_C2hrd"
    },
    {
      "cell_type": "code",
      "source": [
        "stats_por_cluster = df_parking_1.drop('geometria', axis=1).groupby('cluster').mean()\n",
        "\n",
        "stats_por_cluster"
      ],
      "metadata": {
        "id": "hbFyp0zU2hrd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hbFyp0zU2hrd"
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "# 1. Convertir columna de geometría (WKT -> Point)\n",
        "#df_parking_1['geometria'] = df_parking_1['geometria'].apply(wkt.loads)\n",
        "\n",
        "# 2. Crear GeoDataFrame con CRS EPSG:25830\n",
        "gdf = gpd.GeoDataFrame(df_parking_1, geometry='geometria', crs='EPSG:25830')\n",
        "\n",
        "# 3. Reproyectar a EPSG:4326 (lat/lon)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# 4. Extraer latitud y longitud\n",
        "gdf['latitud'] = gdf.geometry.y\n",
        "gdf['longitud'] = gdf.geometry.x\n",
        "\n",
        "# Convertir cluster a string para usar color_discrete_map\n",
        "gdf['cluster_str'] = gdf['cluster'].astype(str)\n",
        "\n",
        "color_discrete_map = {\n",
        "    \"0\": \"green\",\n",
        "    \"1\": \"orange\",\n",
        "    \"2\" : \"red\"\n",
        "}\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf,\n",
        "    lat=\"latitud\",\n",
        "    lon=\"longitud\",\n",
        "    color=\"cluster_str\",  # Usar la columna de strings\n",
        "    zoom=11,\n",
        "    height=600,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    hover_data={\n",
        "        \"media_ocupacion\": True,\n",
        "        \"numero_plazas\": True\n",
        "    },\n",
        "    color_discrete_map=color_discrete_map\n",
        ")\n",
        "\n",
        "# Reducir tamaño de los puntos\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "\n",
        "fig.update_layout(title=\"Clusters de Zonas de Aparcamiento\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "ng7Yjwa-2hrd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ng7Yjwa-2hrd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que muchas de esas zonas verdes se recuperan por la tarde al disminuir el tráfico."
      ],
      "metadata": {
        "id": "17a5qegq2hre"
      },
      "id": "17a5qegq2hre"
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo\n",
        "joblib.dump(kmeans, 'kmeans_tarde_entre_semana.pkl')"
      ],
      "metadata": {
        "id": "6LOTSFmq2hre"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6LOTSFmq2hre"
    },
    {
      "cell_type": "code",
      "source": [
        "df_resumen"
      ],
      "metadata": {
        "id": "iAgrRlRf2hre"
      },
      "execution_count": null,
      "outputs": [],
      "id": "iAgrRlRf2hre"
    },
    {
      "cell_type": "code",
      "source": [
        "df_resumen"
      ],
      "metadata": {
        "id": "PX9_pkrf1bDK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "PX9_pkrf1bDK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí ya leemos el tráfico a tiempo real y montamos el sistema de recomendación utilizando los clusters y el tráfico a tiempo real"
      ],
      "metadata": {
        "id": "R0_r0wIlywee"
      },
      "id": "R0_r0wIlywee"
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://informo.madrid.es/informo/tmadrid/pm.xml\""
      ],
      "metadata": {
        "id": "ueALgrvF5LUP"
      },
      "id": "ueALgrvF5LUP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code != 200:\n",
        "    raise Exception(f\"Error al acceder a la URL: {response.status_code}\")\n",
        "\n",
        "# Parsear XML\n",
        "root = ET.fromstring(response.content)\n",
        "\n",
        "# Crear lista para guardar los datos\n",
        "data = []\n",
        "\n",
        "# Recorremos nodos XML (esto depende de la estructura exacta del XML)\n",
        "for item in root.findall(\".//pm\"):  # Ajusta la etiqueta a tus datos\n",
        "    data.append({\n",
        "        \"descripcion\": item.findtext(\"descripcion\"),\n",
        "        \"carga\": item.findtext(\"carga\"),\n",
        "        \"intensidad\": item.findtext(\"intensidad\"),\n",
        "        \"ocupacion\": item.findtext(\"ocupacion\"),\n",
        "        \"nivelServicio\": item.findtext(\"nivelServicio\"),\n",
        "        \"st_x\": item.findtext(\"st_x\"),\n",
        "        \"st_y\": item.findtext(\"st_y\")\n",
        "    })\n",
        "\n",
        "data = pd.DataFrame(data)\n",
        "data = data.dropna()  # Quitar valores nulos si los hay\n",
        "data[['intensidad', 'ocupacion', 'carga']] = data[['intensidad', 'ocupacion', 'carga']].astype(float)\n"
      ],
      "metadata": {
        "id": "V7CZpjplttQC"
      },
      "id": "V7CZpjplttQC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyproj import Transformer\n",
        "\n",
        "# Define el sistema de coordenadas origen y destino\n",
        "transformer = Transformer.from_crs(\"epsg:32630\", \"epsg:4326\", always_xy=True)\n",
        "# Reemplaza coma por punto y convierte a float\n",
        "data['st_x'] = data['st_x'].str.replace(',', '.').astype(float)\n",
        "data['st_y'] = data['st_y'].str.replace(',', '.').astype(float)\n",
        "\n",
        "lon, lat = transformer.transform(data['st_x'], data['st_y'])\n",
        "data[\"longitud\"] = lon\n",
        "data[\"latitud\"] = lat\n",
        "print(f\"Longitud: {lon}, Latitud: {lat}\")"
      ],
      "metadata": {
        "id": "R8xHowY_tzQx"
      },
      "id": "R8xHowY_tzQx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "min_lat, max_lat = 40.410, 40.440\n",
        "min_lon, max_lon = -3.725, -3.670\n",
        "\n",
        "random_lat = random.uniform(min_lat, max_lat)\n",
        "random_lon = random.uniform(min_lon, max_lon)\n",
        "\n",
        "print(f\"Random location: {random_lat}, {random_lon}\")"
      ],
      "metadata": {
        "id": "FP2a0vkTt4SJ"
      },
      "id": "FP2a0vkTt4SJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parking_pca"
      ],
      "metadata": {
        "id": "yT4LXRtJu2n_"
      },
      "id": "yT4LXRtJu2n_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely import wkt\n",
        "\n",
        "# 1. Convertir columna de geometría (WKT -> Point)\n",
        "# df_parking_1['geometria'] = df_parking_1['geometria'].apply(wkt.loads)\n",
        "\n",
        "# 2. Crear GeoDataFrame con CRS EPSG:25830\n",
        "gdf = gpd.GeoDataFrame(df_parking_1, geometry='geometria', crs='EPSG:25830')\n",
        "\n",
        "# 3. Reproyectar a EPSG:4326 (lat/lon)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# 4. Extraer latitud y longitud\n",
        "gdf['latitud'] = gdf.geometry.y\n",
        "gdf['longitud'] = gdf.geometry.x\n",
        "\n",
        "# Convertir cluster a string para usar color_discrete_map\n",
        "gdf['cluster_str'] = gdf['cluster'].astype(str)\n",
        "\n",
        "color_discrete_map = {\n",
        "    \"0\": \"green\",\n",
        "    \"1\": \"orange\",\n",
        "    \"2\" : \"red\"\n",
        "}\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf,\n",
        "    lat=\"latitud\",\n",
        "    lon=\"longitud\",\n",
        "    color=\"cluster_str\",  # Usar la columna de strings\n",
        "    zoom=11,\n",
        "    height=600,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    hover_data={\n",
        "        \"media_ocupacion\": True,\n",
        "        \"numero_plazas\": True\n",
        "    },\n",
        "    color_discrete_map=color_discrete_map\n",
        ")\n",
        "\n",
        "# Reducir tamaño de los puntos\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "\n",
        "fig.update_layout(title=\"Clusters de Zonas de Aparcamiento\")\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "HQ8qmRWCuET9"
      },
      "id": "HQ8qmRWCuET9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import random\n",
        "\n",
        "fig.add_trace(go.Scattermapbox(\n",
        "    lat=[random_lat],\n",
        "    lon=[random_lon],\n",
        "    mode='markers',\n",
        "    marker=go.scattermapbox.Marker(\n",
        "        size=14,\n",
        "        color='black'\n",
        "    ),\n",
        "    name=\"Random Location\"\n",
        "))\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "pEohla1vuNpl"
      },
      "id": "pEohla1vuNpl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def haversine_np(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius km\n",
        "    phi1 = np.radians(lat1)\n",
        "    phi2 = np.radians(lat2)\n",
        "    d_phi = np.radians(lat2 - lat1)\n",
        "    d_lambda = np.radians(lon2 - lon1)\n",
        "\n",
        "    a = np.sin(d_phi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(d_lambda/2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "\n",
        "    return R * c\n",
        "\n",
        "\n",
        "cluster_df['distance_to_location'] = haversine_np(\n",
        "    cluster_df['latitud'], cluster_df['longitud'], random_lat, random_lon\n",
        ")\n",
        "\n",
        "data['distance_to_location'] = haversine_np(\n",
        "    data['latitud'], cluster_df['longitud'], random_lat, random_lon\n",
        ")\n",
        "\n",
        "radius_km = 1.0\n",
        "nearby_df = cluster_df[cluster_df['distance_to_location'] <= radius_km]\n",
        "nearby_2_df = data[data['distance_to_location'] <= radius_km]"
      ],
      "metadata": {
        "id": "_Lx8hoqgxs4p"
      },
      "id": "_Lx8hoqgxs4p",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
